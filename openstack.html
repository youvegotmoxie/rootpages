

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>OpenStack Queens &mdash; Root Pages 2018.04.01 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="Root Pages 2018.04.01 documentation" href="index.html"/>
        <link rel="next" title="Operating Systems" href="operating_systems.html"/>
        <link rel="prev" title="Networking" href="networking.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> Root Pages
          

          
          </a>

          
            
            
              <div class="version">
                2018.04.01
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Table of Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="ansible.html">Ansible</a></li>
<li class="toctree-l1"><a class="reference internal" href="authentication.html">Authentication</a></li>
<li class="toctree-l1"><a class="reference internal" href="bootloaders.html">Bootloaders</a></li>
<li class="toctree-l1"><a class="reference internal" href="c_and_c++.html">C and C++</a></li>
<li class="toctree-l1"><a class="reference internal" href="databases.html">Databases</a></li>
<li class="toctree-l1"><a class="reference internal" href="clustering.html">Clustering and High Availability</a></li>
<li class="toctree-l1"><a class="reference internal" href="dns_servers.html">DNS</a></li>
<li class="toctree-l1"><a class="reference internal" href="file_systems.html">File Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="http_servers.html">HTTP Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="linux.html">Linux</a></li>
<li class="toctree-l1"><a class="reference internal" href="mail_servers.html">Mail Servers</a></li>
<li class="toctree-l1"><a class="reference internal" href="monitoring.html">Monitoring</a></li>
<li class="toctree-l1"><a class="reference internal" href="networking.html">Networking</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">OpenStack Queens</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#introduction">Introduction</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#versions">Versions</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#red-hat-openstack-platform">Red Hat OpenStack Platform</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#services">Services</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deployment">Deployment</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#packstack">Packstack</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#install">Install</a></li>
<li class="toctree-l4"><a class="reference internal" href="#uninstall">Uninstall</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#openstack-ansible">OpenStack-Ansible</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#quick">Quick</a></li>
<li class="toctree-l4"><a class="reference internal" href="#full">Full</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#tripleo">TripleO</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id4">Quick</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id6">Full</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">Configurations</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#common">Common</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#database">Database</a></li>
<li class="toctree-l4"><a class="reference internal" href="#messaging">Messaging</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#ironic">Ironic</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#drivers">Drivers</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#keystone">Keystone</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#api-v3">API v3</a></li>
<li class="toctree-l4"><a class="reference internal" href="#token-provider">Token Provider</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id11">Nova</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#hypervisors">Hypervisors</a></li>
<li class="toctree-l4"><a class="reference internal" href="#cpu-pinning">CPU Pinning</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id12">Ceph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#nested-virtualization">Nested Virtualization</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#neutron">Neutron</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#network-types">Network Types</a></li>
<li class="toctree-l4"><a class="reference internal" href="#dns">DNS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#metadata">Metadata</a></li>
<li class="toctree-l4"><a class="reference internal" href="#load-balancing-as-a-service">Load-Balancing-as-a-Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="#quality-of-service">Quality of Service</a></li>
<li class="toctree-l4"><a class="reference internal" href="#distributed-virtual-routing">Distributed Virtual Routing</a></li>
<li class="toctree-l4"><a class="reference internal" href="#high-availability">High Availability</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#id15">Ceph</a></li>
<li class="toctree-l3"><a class="reference internal" href="#cinder">Cinder</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id16">Ceph</a></li>
<li class="toctree-l4"><a class="reference internal" href="#encryption">Encryption</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#glance">Glance</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id17">Ceph</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#neutron-troubleshooting">Neutron Troubleshooting</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id18">Open vSwitch</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#floating-ips">Floating IPs</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id19">Upgrades</a></li>
<li class="toctree-l2"><a class="reference internal" href="#command-line-interface-utilities">Command Line Interface Utilities</a></li>
<li class="toctree-l2"><a class="reference internal" href="#orchestration">Orchestration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#heat">Heat</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#resources">Resources</a></li>
<li class="toctree-l4"><a class="reference internal" href="#parameters">Parameters</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#vagrant">Vagrant</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#testing">Testing</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#tempest">Tempest</a></li>
<li class="toctree-l3"><a class="reference internal" href="#rally">Rally</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#installation">Installation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#registering">Registering</a></li>
<li class="toctree-l4"><a class="reference internal" href="#scenarios">Scenarios</a></li>
<li class="toctree-l4"><a class="reference internal" href="#reports">Reports</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performance">Performance</a></li>
<li class="toctree-l2"><a class="reference internal" href="#errata">Errata</a></li>
<li class="toctree-l2"><a class="reference internal" href="#bibliography">Bibliography</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="operating_systems.html">Operating Systems</a></li>
<li class="toctree-l1"><a class="reference internal" href="packages.html">Packages</a></li>
<li class="toctree-l1"><a class="reference internal" href="python.html">Python 3</a></li>
<li class="toctree-l1"><a class="reference internal" href="security.html">Security</a></li>
<li class="toctree-l1"><a class="reference internal" href="virtualization.html">Virtualization</a></li>
<li class="toctree-l1"><a class="reference internal" href="wine.html">Wine</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">Root Pages</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>OpenStack Queens</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/openstack.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="openstack-queens">
<h1><a class="toc-backref" href="#id20">OpenStack Queens</a><a class="headerlink" href="#openstack-queens" title="Permalink to this headline">¶</a></h1>
<div class="contents topic" id="table-of-contents">
<p class="topic-title first">Table of Contents</p>
<ul class="simple">
<li><a class="reference internal" href="#openstack-queens" id="id20">OpenStack Queens</a><ul>
<li><a class="reference internal" href="#introduction" id="id21">Introduction</a><ul>
<li><a class="reference internal" href="#versions" id="id22">Versions</a><ul>
<li><a class="reference internal" href="#red-hat-openstack-platform" id="id23">Red Hat OpenStack Platform</a></li>
</ul>
</li>
<li><a class="reference internal" href="#services" id="id24">Services</a></li>
</ul>
</li>
<li><a class="reference internal" href="#deployment" id="id25">Deployment</a><ul>
<li><a class="reference internal" href="#packstack" id="id26">Packstack</a><ul>
<li><a class="reference internal" href="#install" id="id27">Install</a><ul>
<li><a class="reference internal" href="#answer-file" id="id28">Answer File</a></li>
</ul>
</li>
<li><a class="reference internal" href="#uninstall" id="id29">Uninstall</a></li>
</ul>
</li>
<li><a class="reference internal" href="#openstack-ansible" id="id30">OpenStack-Ansible</a><ul>
<li><a class="reference internal" href="#quick" id="id31">Quick</a><ul>
<li><a class="reference internal" href="#id1" id="id32">Install</a></li>
<li><a class="reference internal" href="#id2" id="id33">Uninstall</a></li>
</ul>
</li>
<li><a class="reference internal" href="#full" id="id34">Full</a><ul>
<li><a class="reference internal" href="#configurations" id="id35">Configurations</a><ul>
<li><a class="reference internal" href="#nova" id="id36">Nova</a></li>
<li><a class="reference internal" href="#ceph" id="id37">Ceph</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id3" id="id38">Install</a></li>
<li><a class="reference internal" href="#operations" id="id39">Operations</a><ul>
<li><a class="reference internal" href="#openstack-utilities" id="id40">OpenStack Utilities</a></li>
<li><a class="reference internal" href="#ansible-inventory" id="id41">Ansible Inventory</a></li>
<li><a class="reference internal" href="#add-a-infrastructure-node" id="id42">Add a Infrastructure Node</a></li>
<li><a class="reference internal" href="#add-a-compute-node" id="id43">Add a Compute Node</a></li>
<li><a class="reference internal" href="#remove-a-compute-node" id="id44">Remove a Compute Node</a></li>
</ul>
</li>
<li><a class="reference internal" href="#upgrades" id="id45">Upgrades</a><ul>
<li><a class="reference internal" href="#minor" id="id46">Minor</a></li>
<li><a class="reference internal" href="#major" id="id47">Major</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#tripleo" id="id48">TripleO</a><ul>
<li><a class="reference internal" href="#id4" id="id49">Quick</a><ul>
<li><a class="reference internal" href="#id5" id="id50">Install</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id6" id="id51">Full</a><ul>
<li><a class="reference internal" href="#id7" id="id52">Install</a><ul>
<li><a class="reference internal" href="#undercloud" id="id53">Undercloud</a></li>
<li><a class="reference internal" href="#overcloud" id="id54">Overcloud</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id8" id="id55">Operations</a><ul>
<li><a class="reference internal" href="#id9" id="id56">Add a Compute Node</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id10" id="id57">Configurations</a><ul>
<li><a class="reference internal" href="#common" id="id58">Common</a><ul>
<li><a class="reference internal" href="#database" id="id59">Database</a></li>
<li><a class="reference internal" href="#messaging" id="id60">Messaging</a></li>
</ul>
</li>
<li><a class="reference internal" href="#ironic" id="id61">Ironic</a><ul>
<li><a class="reference internal" href="#drivers" id="id62">Drivers</a></li>
</ul>
</li>
<li><a class="reference internal" href="#keystone" id="id63">Keystone</a><ul>
<li><a class="reference internal" href="#api-v3" id="id64">API v3</a></li>
<li><a class="reference internal" href="#token-provider" id="id65">Token Provider</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id11" id="id66">Nova</a><ul>
<li><a class="reference internal" href="#hypervisors" id="id67">Hypervisors</a></li>
<li><a class="reference internal" href="#cpu-pinning" id="id68">CPU Pinning</a></li>
<li><a class="reference internal" href="#id12" id="id69">Ceph</a></li>
<li><a class="reference internal" href="#nested-virtualization" id="id70">Nested Virtualization</a></li>
</ul>
</li>
<li><a class="reference internal" href="#neutron" id="id71">Neutron</a><ul>
<li><a class="reference internal" href="#network-types" id="id72">Network Types</a><ul>
<li><a class="reference internal" href="#provider-networks" id="id73">Provider Networks</a><ul>
<li><a class="reference internal" href="#linux-bridge" id="id74">Linux Bridge</a></li>
<li><a class="reference internal" href="#open-vswitch" id="id75">Open vSwitch</a></li>
</ul>
</li>
<li><a class="reference internal" href="#self-service-networks" id="id76">Self-Service Networks</a><ul>
<li><a class="reference internal" href="#id13" id="id77">Linux Bridge</a></li>
<li><a class="reference internal" href="#id14" id="id78">Open vSwitch</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#dns" id="id79">DNS</a></li>
<li><a class="reference internal" href="#metadata" id="id80">Metadata</a></li>
<li><a class="reference internal" href="#load-balancing-as-a-service" id="id81">Load-Balancing-as-a-Service</a></li>
<li><a class="reference internal" href="#quality-of-service" id="id82">Quality of Service</a></li>
<li><a class="reference internal" href="#distributed-virtual-routing" id="id83">Distributed Virtual Routing</a></li>
<li><a class="reference internal" href="#high-availability" id="id84">High Availability</a></li>
</ul>
</li>
<li><a class="reference internal" href="#id15" id="id85">Ceph</a></li>
<li><a class="reference internal" href="#cinder" id="id86">Cinder</a><ul>
<li><a class="reference internal" href="#id16" id="id87">Ceph</a></li>
<li><a class="reference internal" href="#encryption" id="id88">Encryption</a></li>
</ul>
</li>
<li><a class="reference internal" href="#glance" id="id89">Glance</a><ul>
<li><a class="reference internal" href="#id17" id="id90">Ceph</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#neutron-troubleshooting" id="id91">Neutron Troubleshooting</a><ul>
<li><a class="reference internal" href="#id18" id="id92">Open vSwitch</a><ul>
<li><a class="reference internal" href="#floating-ips" id="id93">Floating IPs</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#id19" id="id94">Upgrades</a></li>
<li><a class="reference internal" href="#command-line-interface-utilities" id="id95">Command Line Interface Utilities</a></li>
<li><a class="reference internal" href="#orchestration" id="id96">Orchestration</a><ul>
<li><a class="reference internal" href="#heat" id="id97">Heat</a><ul>
<li><a class="reference internal" href="#resources" id="id98">Resources</a></li>
<li><a class="reference internal" href="#parameters" id="id99">Parameters</a></li>
</ul>
</li>
<li><a class="reference internal" href="#vagrant" id="id100">Vagrant</a></li>
</ul>
</li>
<li><a class="reference internal" href="#testing" id="id101">Testing</a><ul>
<li><a class="reference internal" href="#tempest" id="id102">Tempest</a></li>
<li><a class="reference internal" href="#rally" id="id103">Rally</a><ul>
<li><a class="reference internal" href="#installation" id="id104">Installation</a></li>
<li><a class="reference internal" href="#registering" id="id105">Registering</a></li>
<li><a class="reference internal" href="#scenarios" id="id106">Scenarios</a></li>
<li><a class="reference internal" href="#reports" id="id107">Reports</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#performance" id="id108">Performance</a></li>
<li><a class="reference internal" href="#errata" id="id109">Errata</a></li>
<li><a class="reference internal" href="#bibliography" id="id110">Bibliography</a></li>
</ul>
</li>
</ul>
</div>
<div class="section" id="introduction">
<h2><a class="toc-backref" href="#id21">Introduction</a><a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p>This guide is aimed to help Cloud Administrators through deploying, managing, and upgrading OpenStack.</p>
<div class="section" id="versions">
<h3><a class="toc-backref" href="#id22">Versions</a><a class="headerlink" href="#versions" title="Permalink to this headline">¶</a></h3>
<p>Each OpenStack release starts with a letter, chronologically starting with A. These are usually named after the city where one of the recent development conferences were held. The major version number of OpenStack represents the major version number of each software in that release. For example, Ocata software is versioned as <code class="docutils literal notranslate"><span class="pre">15.X.X</span></code>. A new release comes out after about 6 months of development. After a release, phase 1 of support provides bug fixes for 6 months. Then phase 2 starts for the next 6-12 months that will only provide major bug fixes. Phase 3 only provides security patches for the now end-of-life (EOL) release. Each release is typically supported for 1 year before becoming EOL. [69]</p>
<p>Releases:</p>
<ol class="arabic simple">
<li>Austin</li>
<li>Bexar</li>
<li>Cactus</li>
<li>Diablo</li>
<li>Essex</li>
<li>Folsom</li>
<li>Grizzly</li>
<li>Havana</li>
<li>Icehouse</li>
<li>Juno</li>
<li>Kilo</li>
<li>Liberty</li>
<li>Mitaka</li>
<li>Newton<ul>
<li>Release: 2016-10-06</li>
<li>EOL: 2017-10-11</li>
</ul>
</li>
<li>Ocata<ul>
<li>Release: 2017-02-22</li>
<li>EOL: 2018-02-26 [1]</li>
<li>Goals:<ul>
<li>Stability. This release included features that are mainly related to reliability, scaling, and performance enhancements. This came out 5 months after Newton, instead of the usual 6, due to the minimal amount of major changes. [2]</li>
<li>Remove old OpenStack libraries that were built into some services. Instead, services should rely on the proper up-to-date dependencies provided by external packages. [3]</li>
</ul>
</li>
<li><a class="reference external" href="https://www.openstack.org/news/view/302/openstack-ocata-strengthens-core-infrastructure-services-and-container-integration-with-15th-release-of-cloud-computing-software">New Features</a></li>
</ul>
</li>
<li>Pike<ul>
<li>Release: 2017-08-30</li>
<li>EOL: 2018-09-03 [1]</li>
<li>Goals:<ul>
<li>Convert all of the OpenStack code to be compatible with Python 3. This is because Python 2 will become EOL in 2020.</li>
<li>Make all APIs into WSGI applications. This will allow web servers to scale out and run faster with tuning compared to running as a standalone Python daemon. [4]</li>
</ul>
</li>
<li><a class="reference external" href="https://www.openstack.org/news/view/340/openstack-pike-delivers-composable-infrastructure-services-and-improved-lifecycle-management">New Features</a></li>
</ul>
</li>
<li>Queens<ul>
<li>Release: 2018-02-28</li>
<li>EOL: 2019-02-25</li>
<li>Goals:<ul>
<li>Remove the need for the access control list “policy” files by having default values defined in the source code.</li>
<li>Tempest will be split up into different projects for maintaining individual service unit tests. This contrasts with the old model that had all Tempest tests maintained in one central repository. [5]</li>
</ul>
</li>
<li><a class="reference external" href="https://www.openstack.org/news/view/371/openstack-queens-release-expands-support-for-gpus-and-containers-to-meet-edge-nfv-and-machine-learning-workload-demands">New Features</a></li>
</ul>
</li>
<li>Rocky<ul>
<li>Expected release: 2018-08-30 [1]</li>
</ul>
</li>
</ol>
<div class="section" id="red-hat-openstack-platform">
<h4><a class="toc-backref" href="#id23">Red Hat OpenStack Platform</a><a class="headerlink" href="#red-hat-openstack-platform" title="Permalink to this headline">¶</a></h4>
<p>Red Hat provides most of the development to the core OpenStack services.
The RPM Distribution of OpenStack (RDO) project is a community project
lead by Red Hat to use the latest upstream code from OpenStack and
package it to work and be distributable on Red Hat Enterprise Linux and
Fedora based operating systems. [7]</p>
<p>The Red Hat OpenStack Platform (RHOSP) is a solution by Red Hat that
takes the upstream OpenStack source code and makes it enterprise quality
by hardening the security and increasing it’s stability. Normal releases
are supported for 3 years. Long-life (LL) releases were introduced with
RHOSP 10 where it will receive up to 5 years of support. Every 3rd
release of RHOSP will have LL support. Rolling major upgrades are
supported from one version to the next sequential version, starting with
RHOSP 8.</p>
<p>Releases:</p>
<ul class="simple">
<li>RHOSP 3 (Grizzly)<ul>
<li>Release: 2013-07-10</li>
<li>EOL: 2014-07-31</li>
</ul>
</li>
<li>RHOSP 4 (Havana)<ul>
<li>Release: 2013-12-19</li>
<li>EOL: 2015-06-19</li>
</ul>
</li>
<li>RHOSP 5 (Icehouse)<ul>
<li>Release: 2014-06-30</li>
<li>EOL: 2017-06-30</li>
</ul>
</li>
<li>RHOSP 6 (Juno)<ul>
<li>Release: 2015-02-09</li>
<li>EOL: 2018-02-17</li>
</ul>
</li>
<li>RHOSP 7 (Kilo)<ul>
<li>Release: 2015-08-05</li>
<li>EOL: 2018-08-05</li>
</ul>
</li>
<li>RHOSP 8 (Liberty)<ul>
<li>Release: 2016-04-20</li>
<li>EOL: 2019-04-20</li>
</ul>
</li>
<li>RHOSP 9 (Mitaka)<ul>
<li>Release: 2016-08-24</li>
<li>EOL: 2017-08-24</li>
</ul>
</li>
<li><strong>RHOSP 10 LL (Newton)</strong><ul>
<li>Release: 2016-12-15</li>
<li>EOL: 2021-12-16</li>
</ul>
</li>
<li>RHOSP 11 (Ocata)<ul>
<li>Release: 2017-05-18</li>
<li>EOL: 2018-05-18</li>
</ul>
</li>
<li>RHOSP 12 (Pike)<ul>
<li>Release: 2017-12-13</li>
<li>EOL: 2018-12-13</li>
</ul>
</li>
</ul>
<p>[6]</p>
<p>RHOSP 10 supports these 4 hypervisors [9]:</p>
<ul class="simple">
<li>Kernel-based Virtual Machine (QEMU with KVM acceleration)</li>
<li>Red Hat Enterprise Virtualization (RHEV)</li>
<li>Microsoft Hyper-V</li>
<li>VMWare ESX and ESXi</li>
</ul>
<p>The version of RHOSP in use can be found on the Undercloud by viewing
the “/etc/rhosp-release” file.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ cat /etc/rhosp-release
Red Hat OpenStack Platform release <span class="m">10</span>.0 <span class="o">(</span>Newton<span class="o">)</span>
</pre></div>
</div>
<p>On other nodes, the version can be found by checking the “version” and
“release” of the RPM packages. The version consists of the year and
month of the upstream OpenStack release. The last number in the version
is the bugfix release for this specific package. The release section is
the minor version of the RHOSP. In the example below, the upstream
OpenStack release is Newton that was released on the 10th month of 2016.
The corresponding major RHOSP version is 10 for Newton. This is the 2nd
bugfix release for the package “openstack-nova-common.” The minor
release is 8. The full RHOSP version is referenced as “10z8.” [8]</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ rpm -qi openstack-nova-common
Name        : openstack-nova-common
Version     : <span class="m">2016</span>.10.2
Release     : <span class="m">8</span>.el7ost
...
</pre></div>
</div>
</div>
</div>
<div class="section" id="services">
<h3><a class="toc-backref" href="#id24">Services</a><a class="headerlink" href="#services" title="Permalink to this headline">¶</a></h3>
<p>OpenStack has a large range of services that manage different different
components in a modular way.</p>
<p>Most popular services (50% or more of OpenStack cloud operators have
adopted):</p>
<ul class="simple">
<li>Ceilometer = Telemetry</li>
<li>Cinder = Block Storage</li>
<li>Glance = Image</li>
<li>Heat = Orchestration</li>
<li>Horizon = Dashboard</li>
<li>Keystone = Authentication</li>
<li>Neutron = Networking</li>
<li>Nova = Compute</li>
<li>Swift = Object Storage</li>
</ul>
<p>Other services:</p>
<ul class="simple">
<li>Aodh = Telemetry Alarming</li>
<li>Barbican = Key Management</li>
<li>CloudKitty = Billing</li>
<li>Congress = Governance</li>
<li>Designate = DNS</li>
<li>Freezer = Backup and Recovery</li>
<li>Ironic = Bare-Metal Provisioning</li>
<li>Karbor = Data protection</li>
<li>Kuryr = Container plugin</li>
<li>Magnum = Container Orchestration Engine Provisioning</li>
<li>Manila = Shared File Systems</li>
<li>Mistral = OpenStack Workflow</li>
<li>Monasca = Monitoring</li>
<li>Murano = Application Catalog</li>
<li>Octavia = Load Balancing</li>
<li>Rally = Benchmark</li>
<li>Sahara = Big Data Processing Framework Provisioning</li>
<li>Senlin = Clustering</li>
<li>Solum = Software Development Lifecycle Automation</li>
<li>Searchlight = Indexing</li>
<li>Tacker = NFV Orchestration</li>
<li>Tricircle = Multi-Region Networking Automation</li>
<li>TripleO = Deployment</li>
<li>Trove = Database</li>
<li>Vitrage = Root Cause Analysis</li>
<li>Watcher = Optimization</li>
<li>Zaqar = Messaging</li>
<li>Zun = Containers</li>
</ul>
<p>[10]</p>
</div>
</div>
<div class="section" id="deployment">
<h2><a class="toc-backref" href="#id25">Deployment</a><a class="headerlink" href="#deployment" title="Permalink to this headline">¶</a></h2>
<p>OpenStack can be installed as an all-in-one (AIO) server or onto a cluster of servers. Various tools exist for automating the deployment and management of OpenStack for day 0, 1, and 2 operations.</p>
<div class="section" id="packstack">
<h3><a class="toc-backref" href="#id26">Packstack</a><a class="headerlink" href="#packstack" title="Permalink to this headline">¶</a></h3>
<p>Supported operating system: RHEL/CentOS 7, Fedora</p>
<p>Packstack is part of Red Hat’s RDO project. It’s purpose is for
providing small and simple demonstrations of OpenStack. This tool does
not handle any upgrades of the OpenStack services.</p>
<p>Hardware requirements [25]:</p>
<ul class="simple">
<li>16GB RAM</li>
</ul>
<div class="section" id="install">
<h4><a class="toc-backref" href="#id27">Install</a><a class="headerlink" href="#install" title="Permalink to this headline">¶</a></h4>
<p>First, install the required repositories for OpenStack.</p>
<p>RHEL:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo yum install https://repos.fedorapeople.org/repos/openstack/openstack-queens/rdo-release-queens-1.noarch.rpm
$ sudo subscription-manager repos --enable rhel-7-server-optional-rpms --enable rhel-7-server-extras-rpms
</pre></div>
</div>
<p>CentOS:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo yum install centos-release-openstack-queens
</pre></div>
</div>
<p>Finally, install the Packstack utility.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo yum -y install openstack-packstack
</pre></div>
</div>
<p>There are two network scenarios that Packstack can deploy. The default
is to have an isolated network (1). Floating IPs will not be able to
access the network on the public interface. For lab environments,
Packstack can also configure Neutron to expose the network instead to
allow instances with floating IPs to access other IP addresses on the
network (2).</p>
<p><code class="docutils literal notranslate"><span class="pre">1.</span></code> Isolated Network Install</p>
<p>Generate a configuration file referred to as the “answer” file. This can
optionally be customized. Then install OpenStack using the answer file.
By default, the network will be entirely isolated. [11]</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo packstack --gen-answer-file &lt;FILE&gt;
$ sudo packstack --answer-file &lt;FILE&gt;
</pre></div>
</div>
<p>Packstack logs are stored in /var/tmp/packstack/. The administrator and
demo user credentials will be saved to the user’s home directory.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">source</span> ~/keystonerc_admin
$ <span class="nb">source</span> ~/keystonerc_demo
</pre></div>
</div>
<p>Although the network will not be exposed by default, it can still be
configured later. The primary interface to the lab’s network, typically
<code class="docutils literal notranslate"><span class="pre">eth0</span></code>, will need to be configured as a Open vSwitch bridge to allow
this. Be sure to replace the “IPADDR”, “PREFIX”, and “GATEWAY” with the
server’s correct settings. Neutron will also need to be configured to
allow “flat” networks.</p>
<p>File: /etc/sysconfig/network-scripts/ifcfg-eth0</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEVICE</span><span class="o">=</span><span class="n">eth0</span>
<span class="n">ONBOOT</span><span class="o">=</span><span class="n">yes</span>
<span class="n">DEVICETYPE</span><span class="o">=</span><span class="n">ovs</span>
<span class="n">TYPE</span><span class="o">=</span><span class="n">OVSPort</span>
<span class="n">OVS_BRIDGE</span><span class="o">=</span><span class="n">br</span><span class="o">-</span><span class="n">ex</span>
<span class="n">BOOTPROTO</span><span class="o">=</span><span class="n">none</span>
<span class="n">NM_CONTROLLED</span><span class="o">=</span><span class="n">no</span>
</pre></div>
</div>
<p>File: /etc/sysconfig/network-scripts/ifcfg-br-ex</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">DEVICE</span><span class="o">=</span><span class="n">br</span><span class="o">-</span><span class="n">ex</span>
<span class="n">ONBOOT</span><span class="o">=</span><span class="n">yes</span>
<span class="n">DEVICETYPE</span><span class="o">=</span><span class="n">ovs</span>
<span class="n">TYPE</span><span class="o">=</span><span class="n">OVSBridge</span>
<span class="n">DEFROUTE</span><span class="o">=</span><span class="n">yes</span>
<span class="n">IPADDR</span><span class="o">=</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.200</span>
<span class="n">PREFIX</span><span class="o">=</span><span class="mi">24</span>
<span class="n">GATEWAY</span><span class="o">=</span><span class="mf">192.168</span><span class="o">.</span><span class="mf">1.1</span>
<span class="n">PEERDNS</span><span class="o">=</span><span class="n">no</span>
<span class="n">BOOTPROTO</span><span class="o">=</span><span class="n">none</span>
<span class="n">NM_CONTROLLED</span><span class="o">=</span><span class="n">no</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">2.</span></code> Exposed Network Install</p>
<p>It is also possible to deploy OpenStack where Neutron can have access to
the public network. Run the Packstack installation with the command
below and replace “eth0” with the public interface name.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo packstack --allinone --provision-demo<span class="o">=</span>n --os-neutron-ovs-bridge-mappings<span class="o">=</span>extnet:br-ex --os-neutron-ovs-bridge-interfaces<span class="o">=</span>br-ex:eth0 --os-neutron-ml2-type-drivers<span class="o">=</span>vxlan,flat
</pre></div>
</div>
<p>Alternatively, use these configuration options in the answer file.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">CONFIG_NEUTRON_ML2_TYPE_DRIVERS</span><span class="o">=</span><span class="s">vxlan,flat</span>
<span class="na">CONFIG_NEUTRON_OVS_BRIDGE_MAPPINGS</span><span class="o">=</span><span class="s">extnet:br-ex</span>
<span class="na">CONFIG_NEUTRON_OVS_BRIDGE_IFACES</span><span class="o">=</span><span class="s">br-ex:eth0</span>
<span class="na">CONFIG_PROVISION_DEMO</span><span class="o">=</span><span class="s">n</span>
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo packstack --answer-file &lt;ANSWER_FILE&gt;
</pre></div>
</div>
<p>After the installation is finished, create the necessary network in
Neutron as the admin user. In this example, the network will
automatically allocate IP addresses between 192.168.1.201 and
192.168.1.254. The IP 192.168.1.1 is the router / default gateway.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ . keystonerc_admin
$ neutron net-create external_network --provider:network_type flat --provider:physical_network extnet --router:external
$ neutron subnet-create --name public_subnet --enable_dhcp<span class="o">=</span>False --allocation-pool<span class="o">=</span><span class="nv">start</span><span class="o">=</span><span class="m">192</span>.168.1.201,end<span class="o">=</span><span class="m">192</span>.168.1.254 --gateway<span class="o">=</span><span class="m">192</span>.168.1.1 external_network <span class="m">192</span>.168.1.0/24
</pre></div>
</div>
<p>The “external_network” can now be associated with a router in user
accounts.</p>
<p>[12]</p>
<div class="section" id="answer-file">
<h5><a class="toc-backref" href="#id28">Answer File</a><a class="headerlink" href="#answer-file" title="Permalink to this headline">¶</a></h5>
<p>The “answer” configuration file defines how OpenStack should be setup
and installed. Using a answer file can provide a more customizable
deployment.</p>
<p>Common options:</p>
<ul>
<li><p class="first">CONFIG_DEFAULT_PASSWORD = Any blank passwords in the answer file
will be set to this value.</p>
</li>
<li><p class="first">CONFIG_KEYSTONE_ADMIN_TOKEN = The administrator authentication
token.</p>
</li>
<li><p class="first">CONFIG_KEYSTONE_ADMIN_PW = The administrator password.</p>
</li>
<li><p class="first">CONFIG_MARIADB_PW = The MariaDB root user’s password.</p>
</li>
<li><p class="first">CONFIG_HORIZON_SSL = Configure an SSL for the Horizon dashboard.
This requires that SSLs be generated manually and then defined in the
configuration file [13]:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ for cert in selfcert ssl_dashboard ssl_vnc; do sudo openssl req -x509 -sha256 -newkey rsa:2048 -keyout /etc/pki/tls/private/${cert}.key -out /etc/pki/tls/certs/${cert}.crt -days 365 -nodes; done
</pre></div>
</div>
<ul class="simple">
<li>CONFIG_SSL_CACERT_FILE=/etc/pki/tls/certs/selfcert.crt</li>
<li>CONFIG_SSL_CACERT_KEY_FILE=/etc/pki/tls/private/selfkey.key</li>
<li>CONFIG_VNC_SSL_CERT=/etc/pki/tls/certs/ssl_vnc.crt</li>
<li>CONFIG_VNC_SSL_KEY=/etc/pki/tls/private/ssl_vnc.key</li>
<li>CONFIG_HORIZON_SSL_CERT=/etc/pki/tls/certs/ssl_dashboard.crt</li>
<li>CONFIG_HORIZON_SSL_KEY=/etc/pki/tls/private/ssl_dashboard.key</li>
<li>CONFIG_HORIZON_SSL_CACERT=/etc/pki/tls/certs/selfcert.crt</li>
</ul>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">CONFIG_&lt;SERVICE&gt;_INSTALL</span></code> = Install a specific OpenStack service.</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">CONFIG_&lt;NODE&gt;_HOST</span></code> = The host to setup the relevant services on.</p>
</li>
<li><p class="first"><code class="docutils literal notranslate"><span class="pre">CONFIG_&lt;NODE&gt;_HOSTS</span></code> = A list of hosts to setup the relevant
services on. This currently only exists for “COMPUTE” and “NETWORK.”
New hosts can be added and Packstack re-run to have them added to the
OpenStack cluster.</p>
</li>
<li><p class="first">CONFIG_PROVISION_DEMO = Setup a demo project and user account with
an image and network configured.</p>
</li>
</ul>
</div>
</div>
<div class="section" id="uninstall">
<h4><a class="toc-backref" href="#id29">Uninstall</a><a class="headerlink" href="#uninstall" title="Permalink to this headline">¶</a></h4>
<p>For uninstalling everything that is installed by Packstack, run <a class="reference external" href="https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux_OpenStack_Platform/6/html/Deploying_OpenStack_Proof_of_Concept_Environments/chap-Removing_Packstack_Deployments.html">this Bash script</a> on all of the OpenStack nodes. Use at your own risk.</p>
</div>
</div>
<div class="section" id="openstack-ansible">
<h3><a class="toc-backref" href="#id30">OpenStack-Ansible</a><a class="headerlink" href="#openstack-ansible" title="Permalink to this headline">¶</a></h3>
<p>Supported operating systems: RHEL/CentOS 7, Ubuntu 16.04, openSUSE Leap 42</p>
<p>OpenStack-Ansible uses Ansible for automating the deployment of Ubuntu
inside of LXC containers that run the OpenStack services. This was
created by RackSpace as an official tool for deploying and managing
production environments.</p>
<p>It offers key features that include:</p>
<ul class="simple">
<li>Full LXC containerization of services.</li>
<li>HAProxy load balancing for clustering containers.</li>
<li>Scaling for MariaDB Galera, RabbitMQ, compute nodes, and more.</li>
<li>Central logging with rsyslog.</li>
<li>OpenStack package repository caching.</li>
<li>Automated upgrades.</li>
</ul>
<p>[14]</p>
<div class="section" id="quick">
<h4><a class="toc-backref" href="#id31">Quick</a><a class="headerlink" href="#quick" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id1">
<h5><a class="toc-backref" href="#id32">Install</a><a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h5>
<p>Minimum requirements:</p>
<ul class="simple">
<li>8 CPU cores</li>
<li>50GB storage (80GB recommended)</li>
<li>8GB RAM (16GB recommended)</li>
</ul>
<p>This quick installation guide covers how to install an all-in-one
environment. It is recommended to deploy this inside of a virtual
machine (with nested virtualization enabled) as many system
configurations are changed.</p>
<p>Setup the OpenStack-Ansible project.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo git clone https://git.openstack.org/openstack/openstack-ansible /opt/openstack-ansible
$ <span class="nb">cd</span> /opt/openstack-ansible/
$ sudo git checkout stable/queens
</pre></div>
</div>
<p>There are two all-in-one scenarios that will run different Ansible
Playbooks. The default is “aio” but this can be changed to the second
scenario by setting the <code class="docutils literal notranslate"><span class="pre">SCENARIO</span></code> shell variable to “ceph.”
Alternatively, the roles to run can be manually modified in
<code class="docutils literal notranslate"><span class="pre">/opt/openstack-ansible/tests/bootstrap-aio.yml</span></code> Playbook.</p>
<p><code class="docutils literal notranslate"><span class="pre">$</span> <span class="pre">export</span> <span class="pre">SCENARIO=&quot;ceph&quot;</span></code></p>
<ul class="simple">
<li>aio<ul>
<li>cinder.yml.aio</li>
<li>designate.yml.aio</li>
<li>glance.yml.aio</li>
<li>heat.yml.aio</li>
<li>horizon.yml.aio</li>
<li>keystone.yml.aio</li>
<li>neutron.yml.aio</li>
<li>nova.yml.aio</li>
<li>swift.yml.aio</li>
</ul>
</li>
<li>ceph:<ul>
<li>ceph.yml.aio</li>
<li>cinder.yml.aio</li>
<li>glance.yml.aio</li>
<li>heat.yml.aio</li>
<li>horizon.yml.aio</li>
<li>keystone.yml.aio</li>
<li>neutron.yml.aio</li>
<li>nova.yml.aio</li>
</ul>
</li>
</ul>
<p>Extra Playbooks can be added by copying them from
<code class="docutils literal notranslate"><span class="pre">/opt/openstack-ansible/etc/openstack_deploy/conf.d/</span></code> to
<code class="docutils literal notranslate"><span class="pre">/etc/openstack_deploy/conf.d/</span></code>. The file extensions should be changed
from <code class="docutils literal notranslate"><span class="pre">.yml.aio</span></code> to <code class="docutils literal notranslate"><span class="pre">.yml</span></code> to be correctly parsed.</p>
<p>Then OpenStack-Ansible project can now setup and deploy the LXC containers along with the OpenStack services.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo scripts/bootstrap-ansible.sh
$ sudo scripts/bootstrap-aio.sh
$ <span class="nb">cd</span> /opt/openstack-ansible/playbooks
$ sudo openstack-ansible setup-hosts.yml
$ sudo openstack-ansible setup-infrastructure.yml
$ sudo openstack-ansible setup-openstack.yml
</pre></div>
</div>
<p>If the installation fails, it is recommended to reinstall the operating
system to completely clear out all of the custom configurations that
OpenStack-Ansible creates. Running the <code class="docutils literal notranslate"><span class="pre">scripts/run-playbooks.sh</span></code>
script will not work again until the existing LXC containers and
configurations have been removed. [15]</p>
</div>
<div class="section" id="id2">
<h5><a class="toc-backref" href="#id33">Uninstall</a><a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h5>
<p><a class="reference external" href="https://docs.openstack.org/openstack-ansible/queens/contributor/quickstart-aio.html#rebuilding-an-aio">This Bash script</a> can be used to clean up and uninstall most of the
OpenStack-Ansible installation. Use at your own risk. The recommended
way to uninstall OpenStack-Ansible is to reinstall the operating system. [15]</p>
</div>
</div>
<div class="section" id="full">
<h4><a class="toc-backref" href="#id34">Full</a><a class="headerlink" href="#full" title="Permalink to this headline">¶</a></h4>
<p>Minimum requirements:</p>
<ul class="simple">
<li>3 infrastructure nodes</li>
<li>2 compute nodes</li>
<li>1 log node</li>
</ul>
<p>It is also required to have 4 different network bridges.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">br-mgmt</span></code> = All the nodes should have this network. This is the
management network where all nodes can be accessed and managed by.</li>
<li><code class="docutils literal notranslate"><span class="pre">br-storage</span></code> = This is the only optional interface. It is
recommended to use this to separate the “storage” nodes traffic. This
should exist on the “storage” (when using bare-metal) and “compute”
nodes.</li>
<li><code class="docutils literal notranslate"><span class="pre">br-vlan</span></code> = This should exist on the “network” (when using
bare-metal) and “compute” nodes. It is used for self-service
networks.</li>
<li><code class="docutils literal notranslate"><span class="pre">br-vxlan</span></code> = This should exist on the “network” and “compute”
nodes. It is used for self-service networks.</li>
</ul>
<p>[16]</p>
<div class="section" id="configurations">
<h5><a class="toc-backref" href="#id35">Configurations</a><a class="headerlink" href="#configurations" title="Permalink to this headline">¶</a></h5>
<p>View the
<code class="docutils literal notranslate"><span class="pre">/etc/openstack_deploy/openstack_user_config.yml.prod.example</span></code> for a
real production example and reference.</p>
<p>Configure the networks that are used in the environment.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">cider_networks</span></code><ul>
<li><code class="docutils literal notranslate"><span class="pre">container</span></code> = The network range that the LXC containers will use
an IP address from. This is the management network that is on
“br-mgmt.”</li>
<li><code class="docutils literal notranslate"><span class="pre">tunnel</span></code> = The network range for accessing network services
between the “compute” and “network” nodes over the VXLAN or GRE
tunnel interface. The tunnel network should be on “br-vxlan.”</li>
<li><code class="docutils literal notranslate"><span class="pre">storage</span></code> = The network range for accessing storage. This is the
network that is on “br-storage.”</li>
</ul>
</li>
<li><code class="docutils literal notranslate"><span class="pre">used_ips</span></code> = Lists of IP addresses that are already in use and
should not be used for the container networks.</li>
<li><code class="docutils literal notranslate"><span class="pre">global_overrides</span></code><ul>
<li><code class="docutils literal notranslate"><span class="pre">tunnel_bridge</span></code> = The interface to use for tunneling VXLAN
traffic. This is typically “br-vxlan.”</li>
<li><code class="docutils literal notranslate"><span class="pre">management_bridge</span></code> = The interface to use for management
access. This is typically <code class="docutils literal notranslate"><span class="pre">br-mgmt</span></code>.</li>
<li>external_lb_vip_address = The public IP address to load balance
for API endpoints.</li>
<li><code class="docutils literal notranslate"><span class="pre">provider_networks</span></code><ul>
<li><code class="docutils literal notranslate"><span class="pre">network</span></code> = Different networks can be defined. At least one
is required.<ul>
<li><code class="docutils literal notranslate"><span class="pre">type</span></code> = The type of network that the “container_bridge”
device should be used.<ul>
<li>flat</li>
<li>vlan</li>
<li>vxlan</li>
</ul>
</li>
<li><code class="docutils literal notranslate"><span class="pre">container_bridge</span></code> = The bridge device that will be used
to connect the container to the network. The recommended
deployment scheme recommends setting up a “br-mgmt”,
“br-storage”, “br-vlan”, and “br-vlan.” Any valid bridge
device on the host node can be specified here.</li>
<li><code class="docutils literal notranslate"><span class="pre">container_type</span></code> = veth</li>
<li><code class="docutils literal notranslate"><span class="pre">ip_from_q</span></code> = Specify the “cider_networks” that will be
used to allocate IP addresses from.</li>
<li>range = The optional VXLAN that the bridge interface should
use.</li>
<li><code class="docutils literal notranslate"><span class="pre">container_interface</span></code> = The interface that the LXC
container should use. This is typically “eth1.”</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The syntax for defining which host(s) a service will be installed onto
follow this format below. Controller node services are specified with
the keyword <code class="docutils literal notranslate"><span class="pre">-infra</span></code> in their name. Each <code class="docutils literal notranslate"><span class="pre">infra#</span></code> entry contains the
IP address of the physical server to provision the containers to.</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">&lt;SERVICE_TYPE&gt;</span></code>_hosts:<ul>
<li>infra1:<ul>
<li>ip: <code class="docutils literal notranslate"><span class="pre">&lt;HOST1_IP_ADDRESS&gt;</span></code></li>
</ul>
</li>
<li>infra2:<ul>
<li>ip: <code class="docutils literal notranslate"><span class="pre">&lt;HOST2_IP_ADDRESS&gt;</span></code></li>
</ul>
</li>
<li>infra3:<ul>
<li>ip: <code class="docutils literal notranslate"><span class="pre">&lt;HOST3_IP_ADDRESS&gt;</span></code></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>The valid service types are:</p>
<ul class="simple">
<li>shared-infra = Galera, memcache, RabbitMQ, and other utilities.</li>
<li>repo-infra_hosts = Hosts that will handle storing and retrieving
packages.</li>
<li>metrics = Gnocchi.</li>
<li>metering-alartm_hosts = Aodh.</li>
<li>storage-infra = Cinder.</li>
<li>image = Glance.</li>
<li>identity = Keystone.</li>
<li>haproxy = Load balancers.</li>
<li>log = Central rsyslog servers<ul>
<li><code class="docutils literal notranslate"><span class="pre">log&lt;#&gt;</span></code> = Instead of <code class="docutils literal notranslate"><span class="pre">infra&lt;#&gt;</span></code>, log_hosts uses this
variable for defining the host IP address.</li>
</ul>
</li>
<li>metering-infra = Ceilometer.</li>
<li>metering-alarm = Aodh.</li>
<li>metering-compute = Ceilometer for the compute nodes.</li>
<li>compute-infra = Nova API nodes.</li>
<li>orchestration = Heat.</li>
<li>dashboard = Horizon.</li>
<li>network = Neutron network nodes</li>
<li>compute = Nova hypervisor nodes.</li>
<li>storage = Cinder.</li>
<li>storage-infra</li>
<li>swift = Swift stores.</li>
<li>swift-proxy = Swift proxies.</li>
<li>trove-infra = Trove.</li>
<li>ceph-mon = Ceph monitors.</li>
<li>ceph-osd = Ceph OSDs.</li>
<li>dnsaas = Designate.</li>
<li>unbound = Caching DNS server nodes.</li>
<li>magnum-infra = Magnum.</li>
<li>sahra-infra = Sahara.</li>
</ul>
<p>[16]</p>
<div class="section" id="nova">
<h6><a class="toc-backref" href="#id36">Nova</a><a class="headerlink" href="#nova" title="Permalink to this headline">¶</a></h6>
<p>Common variables:</p>
<ul class="simple">
<li>nova_virt_type = The virtualization technology to use for deploying
instances with OpenStack. By default, OpenStack-Ansible will guess
what should be used based on what is installed on the hypervisor.
Valid options are: <code class="docutils literal notranslate"><span class="pre">qemu</span></code>, <code class="docutils literal notranslate"><span class="pre">kvm</span></code>, <code class="docutils literal notranslate"><span class="pre">lxd</span></code>, <code class="docutils literal notranslate"><span class="pre">ironic</span></code>, or
<code class="docutils literal notranslate"><span class="pre">powervm</span></code>.</li>
</ul>
<p>[17]</p>
</div>
<div class="section" id="ceph">
<h6><a class="toc-backref" href="#id37">Ceph</a><a class="headerlink" href="#ceph" title="Permalink to this headline">¶</a></h6>
<p>Ceph can be customized to be deployed differently from the default
configuration or to use an existing Ceph cluster.</p>
<p>These settings can be adjusted to use different Ceph users, pools,
and/or monitor nodes.</p>
<p>File: /etc/openstack_deploy/user_variables.yml</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">glance_default_store</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">rbd</span>
<span class="l l-Scalar l-Scalar-Plain">glance_ceph_client</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;GLANCE_CEPH_USER&gt;</span>
<span class="l l-Scalar l-Scalar-Plain">glance_rbd_store_pool</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;GLANCE_CEPH_POOL&gt;</span>
<span class="l l-Scalar l-Scalar-Plain">glance_rbd_store_chunk_size</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">8</span>
<span class="l l-Scalar l-Scalar-Plain">cinder_ceph_client</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;CINDER_CEPH_USER&gt;</span>
<span class="l l-Scalar l-Scalar-Plain">nova_ceph_client</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{{</span> <span class="nv">cinder_ceph_client</span> <span class="p p-Indicator">}}</span>
<span class="l l-Scalar l-Scalar-Plain">nova_libvirt_images_rbd_pool</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;CINDER_CEPH_POOL&gt;</span>
<span class="l l-Scalar l-Scalar-Plain">cephx</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
<span class="l l-Scalar l-Scalar-Plain">ceph_mons</span><span class="p p-Indicator">:</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">&lt;MONITOR1_IP&gt;</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">&lt;MONITOR2_IP&gt;</span>
  <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">&lt;MONITOR3_IP&gt;</span>
</pre></div>
</div>
<p>By default, OpenStack-Ansible will generate the ceph.conf configuration
file by connecting to the Ceph monitor hosts and obtaining the
information from there. Extra configuration options can be specified or
overriden using the “ceph_extra”confs” dictionary.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">ceph_extra_confs</span><span class="p p-Indicator">:</span>
<span class="p p-Indicator">-</span>  <span class="l l-Scalar l-Scalar-Plain">src</span><span class="p p-Indicator">:</span> <span class="s">&quot;&lt;PATH_TO_LOCAL_CEPH_CONFIGURATION&gt;&quot;</span>
   <span class="l l-Scalar l-Scalar-Plain">dest</span><span class="p p-Indicator">:</span> <span class="s">&quot;/etc/ceph/ceph.conf&quot;</span>
   <span class="l l-Scalar l-Scalar-Plain">mon_host</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;MONITOR_IP&gt;</span>
   <span class="l l-Scalar l-Scalar-Plain">client_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;CEPH_CLIENT&gt;</span>
   <span class="l l-Scalar l-Scalar-Plain">keyring_src</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;PATH_TO_LOCAL_CEPH_CLIENT_KEYRING_FILE&gt;</span>
   <span class="l l-Scalar l-Scalar-Plain">keyring_dest</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">/etc/ceph/ceph.client.&lt;CEPH_CLIENT&gt;.keyring</span>
   <span class="l l-Scalar l-Scalar-Plain">secret_uuid</span><span class="p p-Indicator">:</span> <span class="s">&#39;{{</span><span class="nv"> </span><span class="s">cinder_ceph_client_&lt;CEPH_CLIENT&gt;</span><span class="nv"> </span><span class="s">}}&#39;</span>
</pre></div>
</div>
<p>Alternatively, the entire configuration file can be defined as a
variable using proper YAML syntax. [19]</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">ceph_conf_file</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">|</span>
  <span class="no">[global]</span>
  <span class="no">fsid = 00000000-1111-2222-3333-444444444444</span>
  <span class="no">mon_initial_members = mon1.example.local,mon2.example.local,mon3.example.local</span>
  <span class="no">mon_host = {{ ceph_mons|join(&#39;,&#39;) }}</span>
  <span class="no">auth_cluster_required = cephx</span>
  <span class="no">auth_service_required = cephx</span>
</pre></div>
</div>
<p>A new custom deployment of Ceph can be configured. It is recommended to
use at least 3 hosts for high availability and quorum. [18]</p>
<p>File: /etc/openstack_deploy/openstack_user_config.yml</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">storage_hosts</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">infra&lt;#&gt;</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">ip</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;CINDER_HOST1_IP&gt;</span>
    <span class="l l-Scalar l-Scalar-Plain">container_vars</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">cinder_backends</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">limit_container_types</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">cinder_volume</span>
        <span class="l l-Scalar l-Scalar-Plain">rbd</span><span class="p p-Indicator">:</span>
          <span class="l l-Scalar l-Scalar-Plain">volume_group</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;LVM_BLOCK_STORAGE&gt;</span>
          <span class="l l-Scalar l-Scalar-Plain">volume_driver</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">cinder.volume.drivers.rbd.RBDDriver</span>
          <span class="l l-Scalar l-Scalar-Plain">volume_backend_name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">rbd</span>
          <span class="l l-Scalar l-Scalar-Plain">rbd_pool</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;CINDER_CEPH_POOL&gt;</span>
          <span class="l l-Scalar l-Scalar-Plain">rbd_ceph_conf</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">/etc/ceph/ceph.conf</span>
          <span class="l l-Scalar l-Scalar-Plain">rbd_user</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;CINDER_CEPHX_USER&gt;</span>
</pre></div>
</div>
<p>[18]</p>
<p>Another real-world example of deploying and managing Ceph as part of
OpenStack-Ansible can be found here:
<a class="reference external" href="https://github.com/openstack/openstack-ansible/commit/057bb30547ef753b4559a689902be711b83fd76f">https://github.com/openstack/openstack-ansible/commit/057bb30547ef753b4559a689902be711b83fd76f</a></p>
</div>
</div>
<div class="section" id="id3">
<h5><a class="toc-backref" href="#id38">Install</a><a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h5>
<p>Download and install the latest stable OpenStack-Ansible suite from
GitHub.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo git clone https://git.openstack.org/openstack/openstack-ansible /opt/openstack-ansible
$ <span class="nb">cd</span> /opt/openstack-ansible/
$ sudo git checkout stable/queens
$ sudo cp -a -r -v /opt/openstack-ansible/etc/openstack_deploy/ /etc/
</pre></div>
</div>
<p>Install Ansible and the related OpenStack Roles.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo /opt/openstack-ansible/scripts/bootstrap-ansible.sh
</pre></div>
</div>
<p>Generate random passwords for the services.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo /opt/openstack-ansible/scripts/pw-token-gen.py --file /etc/openstack_deploy/user_secrets.yml
</pre></div>
</div>
<ul class="simple">
<li>Configure OSA and verify that the configuration syntax is correct.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo cp /etc/openstack_deploy/openstack_user_config.yml.example /etc/openstack_deploy/openstack_user_config.yml
$ sudo vim /etc/openstack_deploy/openstack_user_config.yml
$ sudo openstack-ansible setup-infrastructure.yml --syntax-check
</pre></div>
</div>
<ul class="simple">
<li>Prepare the hosts.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible setup-hosts.yml
</pre></div>
</div>
<ul class="simple">
<li>Setup the LXC containers.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible setup-infrastructure.yml
</pre></div>
</div>
<ul class="simple">
<li>Install the OpenStack services.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible setup-openstack.yml
</pre></div>
</div>
<p>[16]</p>
</div>
<div class="section" id="operations">
<h5><a class="toc-backref" href="#id39">Operations</a><a class="headerlink" href="#operations" title="Permalink to this headline">¶</a></h5>
<div class="section" id="openstack-utilities">
<h6><a class="toc-backref" href="#id40">OpenStack Utilities</a><a class="headerlink" href="#openstack-utilities" title="Permalink to this headline">¶</a></h6>
<p>Once OpenStack-Ansible is installed, it can be used immediately. The
primary container to use is the <code class="docutils literal notranslate"><span class="pre">utility</span></code> container.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo lxc-ls -1 <span class="p">|</span> grep utility
$ sudo lxc-attach -n &lt;UTILITY_CONTAINER_NAME&gt;
</pre></div>
</div>
<p>The file <code class="docutils literal notranslate"><span class="pre">/root/openrc</span></code> should exist on the container with the
administrator credentials. Source this file to use them.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">source</span> /root/openrc
</pre></div>
</div>
<p>Verify that all of the correct services and endpoints exist.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack service list
$ openstack endpoint list
</pre></div>
</div>
<p>[20]</p>
</div>
<div class="section" id="ansible-inventory">
<h6><a class="toc-backref" href="#id41">Ansible Inventory</a><a class="headerlink" href="#ansible-inventory" title="Permalink to this headline">¶</a></h6>
<p>Ansible’s inventory contains all of the connection and variable details
about the hosts (in this case, LXC containers) and which group they are
a part of. This section covers finding and using these inventory values
for management and troubleshooting.</p>
<ul>
<li><p class="first">Change into the OpenStack-Ansible directory.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> /opt/openstack-ansible/
</pre></div>
</div>
</li>
<li><p class="first">Show all of the groups and the hosts that are a part of it.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ./scripts/inventory-manage.py -G
</pre></div>
</div>
</li>
<li><p class="first">Show all of the hosts and the groups they are a part of.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ./scripts/inventory-manage.py -g
</pre></div>
</div>
</li>
<li><p class="first">List hosts that a Playbook will run against.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible ./playbooks/os-&lt;COMPONENT&gt;-install.yml --limit &lt;GROUP&gt; --list-hosts
</pre></div>
</div>
</li>
<li><p class="first">List all the Ansible tasks that will be executed on a group or host.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible ./playbooks/os-&lt;COMPONENT&gt;-install.yml --limit &lt;GROUP_OR_HOST&gt; --list-tasks
</pre></div>
</div>
</li>
</ul>
<p>[21]</p>
</div>
<div class="section" id="add-a-infrastructure-node">
<h6><a class="toc-backref" href="#id42">Add a Infrastructure Node</a><a class="headerlink" href="#add-a-infrastructure-node" title="Permalink to this headline">¶</a></h6>
<p>Add the new host to the <code class="docutils literal notranslate"><span class="pre">infra_hosts</span></code> section in
<code class="docutils literal notranslate"><span class="pre">/etc/openstack_deploy/openstack_user_config.yml</span></code>. Then the inventory
can be updated which will generate a new unique node name that the
OpenStack-Ansible Playbooks can run against. The <code class="docutils literal notranslate"><span class="pre">--limit</span></code> options are
important because they will ensure that it will only run on the new
infrastructure node.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> /opt/openstack-ansible/playbooks
$ sudo /opt/openstack-ansible/playbooks/inventory/dynamic_inventory.py &gt; /dev/null
$ sudo /opt/openstack-ansible/scripts/inventory-manage.py -l <span class="p">|</span>awk <span class="s1">&#39;/&lt;NEW_INFRA_HOST&gt;/ {print $2}&#39;</span> <span class="p">|</span> sort -u <span class="p">|</span> tee /root/add_host.limit
$ sudo openstack-ansible setup-everything.yml --limit @/root/add_host.limit
$ sudo openstack-ansible --tags<span class="o">=</span>openstack-host-hostfile setup-hosts.yml
</pre></div>
</div>
<p>[20]</p>
</div>
<div class="section" id="add-a-compute-node">
<h6><a class="toc-backref" href="#id43">Add a Compute Node</a><a class="headerlink" href="#add-a-compute-node" title="Permalink to this headline">¶</a></h6>
<p>Add the new host to the <code class="docutils literal notranslate"><span class="pre">compute_hosts</span></code> section in
<code class="docutils literal notranslate"><span class="pre">/etc/openstack_deploy/openstack_user_config.yml</span></code>. Then the
OpenStack-Ansible deployment Playbooks can be run again. If Ceilometer is in use then the `` /etc/openstack_deploy/conf.d/ceilometer.yml`` configuration will also have to be updated.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> /opt/openstack-ansible/playbooks
$ sudo openstack-ansible setup-hosts.yml --limit localhost,&lt;NEW_COMPUTE_HOST&gt;
$ sudo ansible nova_all -m setup -a <span class="s1">&#39;filter=ansible_local gather_subset=&quot;!all&quot;&#39;</span>
$ sudo openstack-ansible setup-openstack.yml --skip-tags nova-key-distribute --limit localhost,&lt;NEW_COMPUTE_HOST&gt;
$ sudo openstack-ansible setup-openstack.yml --tags nova-key --limit compute_hosts
</pre></div>
</div>
<p>[20]</p>
</div>
<div class="section" id="remove-a-compute-node">
<h6><a class="toc-backref" href="#id44">Remove a Compute Node</a><a class="headerlink" href="#remove-a-compute-node" title="Permalink to this headline">¶</a></h6>
<p>Stop the services on the compute container and then use the
<code class="docutils literal notranslate"><span class="pre">openstack-ansible-ops</span></code> project’s Playbook <code class="docutils literal notranslate"><span class="pre">remote_compute_node.yml</span></code>
to fully it. The host must also be removed from the
<code class="docutils literal notranslate"><span class="pre">/etc/openstack_deploy/openstack_user_config.yml</span></code> configuration when
done.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo lxc-ls -1 <span class="p">|</span> grep compute
$ sudo lxc-attach -n &lt;COMPUTE_CONTAINER_TO_REMOVE&gt;
$ sudo stop nova-compute
$ sudo stop neutron-linuxbridge-agent
$ <span class="nb">exit</span>
$ sudo git clone https://git.openstack.org/openstack/openstack-ansible-ops /opt/openstack-ansible-ops
$ <span class="nb">cd</span> /opt/openstack-ansible-ops/ansible_tools/playbooks
$ sudo openstack-ansible remove_compute_node.yml -e <span class="nv">node_to_be_removed</span><span class="o">=</span><span class="s2">&quot;&lt;COMPUTE_CONTAINER_TO_REMOVE&gt;&quot;</span>
</pre></div>
</div>
<p>[20]</p>
</div>
</div>
<div class="section" id="upgrades">
<h5><a class="toc-backref" href="#id45">Upgrades</a><a class="headerlink" href="#upgrades" title="Permalink to this headline">¶</a></h5>
<div class="section" id="minor">
<h6><a class="toc-backref" href="#id46">Minor</a><a class="headerlink" href="#minor" title="Permalink to this headline">¶</a></h6>
<p>This is for upgrading OpenStack from one minor version to another in the
same major release. An example would be going from 17.0.0 to 17.1.1.</p>
<ul>
<li><p class="first">Change the OpenStack-Ansible version to a new minor tag release. If a
branch for a OpenStack release name is being used already, pull the
latest branch commits down from GitHub.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> /opt/openstack-ansible/
$ sudo git fetch --all
$ sudo git checkout &lt;TAG&gt;
</pre></div>
</div>
</li>
<li><p class="first">Update:</p>
<ul>
<li><p class="first"><strong>All services.</strong></p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ./scripts/bootstrap-ansible.sh
$ <span class="nb">cd</span> ./playbooks/
$ sudo openstack-ansible setup-hosts.yml
$ sudo openstack-ansible -e <span class="nv">rabbitmq_upgrade</span><span class="o">=</span><span class="nb">true</span> setup-infrastructure.yml
$ sudo openstack-ansible setup-openstack.yml
</pre></div>
</div>
</li>
<li><p class="first"><strong>Specific services.</strong></p>
<ul>
<li><p class="first">Update the cached package repository.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> ./playbooks/
$ sudo openstack-ansible repo-install.yml
</pre></div>
</div>
</li>
<li><p class="first">A single service can be upgraded now.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible &lt;COMPONENT&gt;-install.yml --limit &lt;GROUP_OR_HOST&gt;
</pre></div>
</div>
</li>
<li><p class="first">Some services, such as MariaDB and RabbitMQ, require special
variables to be set to force an upgrade.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible galera-install.yml -e <span class="s1">&#39;galera_upgrade=true&#39;</span>
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo openstack-ansible rabbitmq-install.yml -e <span class="s1">&#39;rabbitmq_upgrade=true&#39;</span>
</pre></div>
</div>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>[22]</p>
</div>
<div class="section" id="major">
<h6><a class="toc-backref" href="#id47">Major</a><a class="headerlink" href="#major" title="Permalink to this headline">¶</a></h6>
<p>OpenStack-Ansible has scripts capable of fully upgrading OpenStack from
one major release to the next. It is recommended to do a manual upgrade
by following the <a class="reference external" href="https://docs.openstack.org/openstack-ansible/queens/user/manual-upgrade.html">official guide</a>
Below outlines how to do this automatically. [22]</p>
<ul>
<li><p class="first">Move into the OpenStack-Ansible project.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">cd</span> /opt/openstack-ansible
</pre></div>
</div>
</li>
<li><p class="first">View the available OpenStack releases and choose which one to use.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo git branch -a
$ sudo git tag
$ sudo git checkout &lt;BRANCH_OR_TAG&gt;
</pre></div>
</div>
</li>
<li><p class="first">Run the upgrade script.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ./scripts/run-upgrade.sh
</pre></div>
</div>
</li>
</ul>
</div>
</div>
</div>
</div>
<div class="section" id="tripleo">
<h3><a class="toc-backref" href="#id48">TripleO</a><a class="headerlink" href="#tripleo" title="Permalink to this headline">¶</a></h3>
<p>Supported operating systems: RHEL 7, Fedora &gt;= 22</p>
<p>TripleO means “OpenStack on OpenStack.” The Undercloud is first deployed in a small, usually all-in-one, environment. This server is then used to create and manage a full Overcloud cluster.</p>
<p>In Pike, most of the Overcloud can be deployed into docker containers built by Kolla. The most notable service that lacked container support was Neutron due to it’s complexity. Starting in Queens, all of the Overcloud services can now be installed as docker containers. There is also experimental support for running the Undercloud services in containers. [81]</p>
<p>Hardware requirements [24]:</p>
<ul class="simple">
<li>Undercloud node:<ul>
<li>4 CPU cores</li>
<li>16GB RAM</li>
<li>60GB storage</li>
</ul>
</li>
<li>Overcloud nodes:<ul>
<li>4 CPU cores</li>
<li>8GB RAM</li>
<li>80GB storage</li>
</ul>
</li>
</ul>
<div class="section" id="id4">
<h4><a class="toc-backref" href="#id49">Quick</a><a class="headerlink" href="#id4" title="Permalink to this headline">¶</a></h4>
<p>The “TripleO-Quickstart” project was created to use Ansible to automate
deploying TripleO as fast and easily as possible. [23]</p>
<div class="section" id="id5">
<h5><a class="toc-backref" href="#id50">Install</a><a class="headerlink" href="#id5" title="Permalink to this headline">¶</a></h5>
<p>TripleO-Quickstart recommends a minimum of 32GB RAM and 120GB of disk
space when deploying with the default settings. [25] This deployment has
to use a baremetal hypervisor. Deploying TripleO within a virtual
machine that uses nested virtualization is not supported. [26]</p>
<ul>
<li><p class="first">Download the tripleo-quickstart script or clone the entire repository
from GitHub.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ curl -O https://raw.githubusercontent.com/openstack/tripleo-quickstart/master/quickstart.sh
</pre></div>
</div>
<p>OR</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ git clone https://github.com/openstack/tripleo-quickstart.git
$ <span class="nb">cd</span> tripleo-quickstart
</pre></div>
</div>
</li>
<li><p class="first">Install dependencies for the quickstart script.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ bash quickstart.sh --install-deps
</pre></div>
</div>
</li>
</ul>
<p>TripleO can now be installed automatically with the default setup of 3
virtual machines. This will be created to meet the minimum TripleO cloud
requirements: (1) an Undercloud to deploy a (2) controller and (3)
compute node. [24] . Otherwise, a different node configuration from
“config/nodes/” can be specified or created.</p>
<p>Common node variables:</p>
<ul class="simple">
<li>{block|ceph|compute|control|default|objectstorage|undercloud}_{memory|vcpu}
= Define the amount of processor cores or RAM (in megabytes) to
allocate to the respective virtual machine type. Use “default” to
apply to all nodes that are not explicitly defined.</li>
</ul>
<p>Further customizations should be configured now before deploying the
TripleO environment. Refer to the <a class="reference external" href="https://github.com/openstack/tripleo-quickstart-extras/blob/master/roles/undercloud-deploy/README.md">Undercloud Deploy role’s
documentation</a>
on all of the Ansible variables for the Undercloud. Add any override
variables to a YAML file and then add the arguments
<code class="docutils literal notranslate"><span class="pre">-e</span> <span class="pre">&#64;&lt;VARIABLE_FILE&gt;.yaml</span></code> to the “quickstart.sh” commands.</p>
<p><code class="docutils literal notranslate"><span class="pre">1.</span></code> Automatic</p>
<ul class="simple">
<li>Run the quickstart script to install TripleO. Use “127.0.0.2” for the
localhost IP address if TripleO will be installed on the same system
that the quickstart command is running on.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ bash quickstart.sh --release trunk/queens --tags all &lt;REMOTE_HYPERVISOR_IP&gt;
</pre></div>
</div>
<p>[23]</p>
<p><code class="docutils literal notranslate"><span class="pre">2.</span></code> Manual</p>
<ul class="simple">
<li>Common quickstart.sh options:<ul>
<li><code class="docutils literal notranslate"><span class="pre">--clean</span></code> = Remove previously created files from the working
directory on the start of TripleO-Quickstart.</li>
<li><code class="docutils literal notranslate"><span class="pre">--no-clone</span></code> = Use the current working directory for
TripleO-Quickstart. This should only be if the entire repository
has been cloned.</li>
<li><code class="docutils literal notranslate"><span class="pre">--nodes</span> <span class="pre">config/nodes/&lt;CONFIGURATION&gt;.yml</span></code> = Specify the
configuration that determines how many Overcloud nodes should be
deployed.</li>
<li><code class="docutils literal notranslate"><span class="pre">-p</span></code> = Specify a Playbook to run.</li>
<li><code class="docutils literal notranslate"><span class="pre">--release</span></code> = The OpenStack release to use. All of the available
releases can be found in the GitHub project in the
“config/release/” directory. Use “trunk/<code class="docutils literal notranslate"><span class="pre">&lt;RELEASE_NAME&gt;</span></code>” for
the development version and “stable/<code class="docutils literal notranslate"><span class="pre">&lt;RELEASE_NAME&gt;</span></code>” for the
stable version.</li>
<li><code class="docutils literal notranslate"><span class="pre">--retain-inventory</span></code> = Use the existing inventory. This is
useful for managing an existing TripleO-Quickstart infrastructure.</li>
<li><code class="docutils literal notranslate"><span class="pre">--teardown</span> <span class="pre">{all|nodes|none|virthost}</span></code> = Delete everything
related to TripleO (all), only the virtual machines (nodes),
nothing (none), or the virtual machines and settings on the
hypervisor (virthost).</li>
<li><code class="docutils literal notranslate"><span class="pre">--tags</span> <span class="pre">all</span></code> = Deploy a complete all-in-one TripleO installation
automatically. If a Playbook is specified via <code class="docutils literal notranslate"><span class="pre">-p</span></code>, then
everything in that Playbook will run.</li>
<li><code class="docutils literal notranslate"><span class="pre">-v</span></code> = Show verbose output from the Ansible Playbooks.</li>
<li><code class="docutils literal notranslate"><span class="pre">--config=~/.quickstart/config/general_config/containers_minimal.yml</span></code> = Deploy the Overcloud from Kolla docker containers. [81]</li>
</ul>
</li>
</ul>
<hr class="docutils" />
<ul>
<li><p class="first">Setup the Undercloud virtual machine.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ bash quickstart.sh --release trunk/queens --clean --teardown all --tags all --playbook quickstart.yml &lt;REMOTE_HYPERVISOR_IP&gt;
</pre></div>
</div>
</li>
<li><p class="first">Install the Undercloud services.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ bash quickstart.sh --release trunk/queens --teardown none --no-clone --tags all --retain-inventory --playbook quickstart-extras-undercloud.yml &lt;REMOTE_HYPERVISOR_IP&gt;
</pre></div>
</div>
</li>
<li><p class="first">Setup the Overcloud virtual machines.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ bash quickstart.sh --release trunk/queens --teardown none --no-clone --tags all --nodes config/nodes/1ctlr_1comp.yml --retain-inventory --playbook quickstart-extras-overcloud-prep.yml &lt;REMOTE_HYPERVISOR_IP&gt;
</pre></div>
</div>
</li>
<li><p class="first">Install the Overcloud services.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ bash quickstart.sh --release trunk/queens --teardown none --no-clone --tags all --nodes config/nodes/1ctlr_1comp.yml --retain-inventory --playbook quickstart-extras-overcloud.yml &lt;REMOTE_HYPERVISOR_IP&gt;
</pre></div>
</div>
</li>
<li><p class="first">Validate the installation.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ bash quickstart.sh --release trunk/queens --teardown none --no-clone --tags all --nodes config/nodes/1ctlr_1comp.yml --retain-inventory  --playbook quickstart-extras-validate.yml &lt;REMOTE_HYPERVISOR_IP&gt;
</pre></div>
</div>
</li>
</ul>
<p>[27]</p>
</div>
</div>
<div class="section" id="id6">
<h4><a class="toc-backref" href="#id51">Full</a><a class="headerlink" href="#id6" title="Permalink to this headline">¶</a></h4>
<div class="section" id="id7">
<h5><a class="toc-backref" href="#id52">Install</a><a class="headerlink" href="#id7" title="Permalink to this headline">¶</a></h5>
<div class="section" id="undercloud">
<h6><a class="toc-backref" href="#id53">Undercloud</a><a class="headerlink" href="#undercloud" title="Permalink to this headline">¶</a></h6>
<p>The Undercloud can be installed onto a bare metal server or a virtual
machine. Follow the “hypervisor” section to assist with automatically
creating an Undercloud virtual machine.</p>
<ul>
<li><p class="first"><strong>Hypervisor</strong> (optional)</p>
<ul>
<li><p class="first">Install the RDO Trunk / Delorean repositories.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo curl -L -o /etc/yum.repos.d/delorean-queens.repo https://trunk.rdoproject.org/centos7-queens/current/delorean.repo
$ sudo curl -L -o /etc/yum.repos.d/delorean-deps-queens.repo https://trunk.rdoproject.org/centos7-queens/delorean-deps.repo
</pre></div>
</div>
</li>
<li><p class="first">Install the Undercloud environment deployment tools.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo yum install instack-undercloud
</pre></div>
</div>
</li>
<li><p class="first">Deploy a new virtual machine to be used for the Undercloud.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ instack–virt–setup
</pre></div>
</div>
</li>
<li><p class="first">Alternatively, use the TripleO-Quickstart project to deploy the
Undercloud virtual machine. Leave the overcloud_nodes variable
blank to only deploy the Undercloud. Otherwise, provide a number
of virtual machines that should be created for use in the
Overcloud.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ curl -O https://raw.githubusercontent.com/openstack/tripleo-quickstart/master/quickstart.sh
$ bash quickstart.sh --tags all --playbook quickstart.yml -e <span class="nv">overcloud_nodes</span><span class="o">=</span><span class="s2">&quot;&quot;</span> <span class="nv">$VIRTHOST</span>
</pre></div>
</div>
</li>
<li><p class="first">Log into the virtual machine once TripleO-Quickstart has completed
setting up the environment.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ ssh -F ~/.quickstart/ssh.config.ansible undercloud
</pre></div>
</div>
</li>
</ul>
</li>
<li><p class="first"><strong>Undercloud</strong></p>
<ul>
<li><p class="first">It is recommended to create a user named “stack” with sudo
privileges to manage the Undercloud.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo useradd stack
$ sudo passwd stack
$ sudo <span class="nb">echo</span> <span class="s2">&quot;stack ALL=(root) NOPASSWD:ALL&quot;</span> <span class="p">|</span> tee -a /etc/sudoers.d/stack
$ sudo chmod <span class="m">0440</span> /etc/sudoers.d/stack
$ sudo su - stack
</pre></div>
</div>
</li>
<li><p class="first">Install the RDO Trunk repositories.</p>
</li>
<li><p class="first">Install TripleO.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo yum install python-tripleoclient
</pre></div>
</div>
</li>
<li><p class="first">Copy the sample configuration to use as a base template.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ cp /usr/share/instack-undercloud/undercloud.conf.sample ~/undercloud.conf
</pre></div>
</div>
</li>
<li><p class="first">Common Undercloud configuration options:</p>
<ul class="simple">
<li>enable_* = Enable or disable non-essential OpenStack services
on the Undercloud.</li>
<li>dhcp_{start|end} = The range of IP addresses to temporarily
use for provisioning Overcloud nodes. This range is a limiting
factor in how many nodes can be provisioned at once.</li>
<li>local_interface = The network interface to use for
provisioning new Overcloud nodes. This will be configured as an
Open vSwitch bridge.</li>
<li>local_mtu = The MTU size to use for the local interface.</li>
<li>network_cidr = The CIDR range of IP addresses to temporarily
use for provisioning.</li>
<li>masquerade_network = The network CIDR that will be used for
masquerading external network connections.</li>
<li>network_gateway = The default gateway to use for external
connectivity to the Internet during provisioning.</li>
<li>undercloud_admin_vip = The IP address to listen on for admin
API endpoints.</li>
<li>undercloud_hostname = The fully qualified hostname to use for
the Undercloud.</li>
<li>undercloud_public_vip = The IP address to listen on for
public API endpoints.</li>
</ul>
</li>
<li><p class="first">At the very least the “local_ip” and “local_interface” variables
need to be defined in the “DEFAULT” section.</p>
</li>
<li><p class="first">Deploy an all-in-one Undercloud on the virtual machine.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack undercloud install
</pre></div>
</div>
</li>
<li><p class="first">The installation will be logged to
<code class="docutils literal notranslate"><span class="pre">$HOME/.instack/install-undercloud.log</span></code>.</p>
</li>
<li><p class="first">After the installation, OpenStack user credentials will be saved
to <code class="docutils literal notranslate"><span class="pre">$HOME/stackrc</span></code>. Source this file before running OpenStack
commands to verify that the Undercloud is operational.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">source</span> ~/stackrc
$ openstack catalog list
</pre></div>
</div>
</li>
<li><p class="first">All OpenStack service passwords will be saved to
<code class="docutils literal notranslate"><span class="pre">$HOME/undercloud-passwords.conf</span></code>.</p>
</li>
</ul>
</li>
</ul>
<p>[28]</p>
</div>
<div class="section" id="overcloud">
<h6><a class="toc-backref" href="#id54">Overcloud</a><a class="headerlink" href="#overcloud" title="Permalink to this headline">¶</a></h6>
<ul>
<li><p class="first">Download the prebuilt Overcloud image files from
<a class="reference external" href="https://images.rdoproject.org/">https://images.rdoproject.org/</a></p>
<ul class="simple">
<li>ironic-python-agent.initramfs</li>
<li>ironic-python-agent.kernel</li>
<li>overcloud-full.initrd</li>
<li>overcloud-full.qcow2</li>
<li>overcloud-full.vmlinuz</li>
</ul>
</li>
<li><p class="first">Upload those images.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack overcloud image upload
</pre></div>
</div>
</li>
<li><p class="first">Create a “instackenv.json” file that describes the physical infrastructure of the Overcloud as <a class="reference external" href="https://docs.openstack.org/tripleo-docs/latest/install/environments/baremetal.html#instackenv">outlined here</a>. By default Ironic manages rebooting machines using the IPMI “pxe_ipmitool” driver. [75]</p>
<blockquote>
<div><ul>
<li><p class="first">Virtual lab environment notes:</p>
<blockquote>
<div><ul class="simple">
<li>The “pxe_fake” driver can be used. This will require the end-user to manually reboot the managed nodes.</li>
<li>Alternatively, VirtualBMC can be used to emulate IPMI with Libvirt. [76]</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo yum install -y python-virtualbmc
$ vbmc add &lt;VM_NAME&gt; --port &lt;IPMI_PORT&gt; --username admin --password password
$ vbmc start &lt;VM_NAME&gt;
$ <span class="nb">echo</span> <span class="s2">&quot;Verifying that IPMI now works.&quot;</span>
$ ipmitool -I lanplus -U admin -P password -H <span class="m">127</span>.0.0.1 -p &lt;IPMI_PORT&gt; power on
</pre></div>
</div>
<ul>
<li><p class="first">Import the configuration that defines the Overcloud infrastructure
and have it introspected so it can be deployed:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack overcloud node import --introspect --provide instackenv.json
</pre></div>
</div>
<ul>
<li><p class="first">Alternatively, automatically discover the available servers by
scanning IPMI devices via a CIDR range and using different IPMI
logins.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack overcloud node discover --range &lt;CIDR&gt; <span class="se">\</span>
--credentials &lt;USER1&gt;:&lt;PASSWORD1&gt; --credentials &lt;USER2&gt;:&lt;PASSWORD2&gt;
</pre></div>
</div>
</li>
</ul>
</li>
<li><p class="first">Deploy the Overcloud with any custom Heat configurations. [29] Starting with the Pike release, most services are deployed as containers by default. For preventing the use of containers, remove the “docker.yaml” and “docker-ha.yaml” files from <cite>/usr/share/openstack-tripleo-heat-templates/environments/</cite>. [30]</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack <span class="nb">help</span> overcloud deploy
</pre></div>
</div>
</li>
<li><p class="first">Optionally for container support, configure the upstream RDO Docker Hub repository to download containers from. Then reference the docker, docker-ha, and docker_registry templates. The “environments/puppet-pacemaker.yaml” template should also be removed to avoid conflicts.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack overcloud container image prepare --namespace docker.io/tripleomaster --tag current-tripleo --tag-from-label rdo_version --output-env-file ~/docker_registry.yaml
$ openstack overcloud deploy &lt;DEPLOY_OPTIONS&gt; -e /usr/share/openstack-tripleo-heat-templates/environments/docker.yaml -e ~/docker_registry.yaml -e /usr/share/openstack-tripleo-heat-templates/environments/docker-ha.yaml
</pre></div>
</div>
</li>
<li><p class="first">Verify that the Overcloud was deployed.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack stack list
$ openstack stack show &lt;OVERCLOUD_STACK_ID&gt;
</pre></div>
</div>
</li>
<li><p class="first">Source the Overcloud credentials to manage it.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">source</span> ~/overcloudrc
</pre></div>
</div>
</li>
</ul>
<p>[29]</p>
</div>
</div>
<div class="section" id="id8">
<h5><a class="toc-backref" href="#id55">Operations</a><a class="headerlink" href="#id8" title="Permalink to this headline">¶</a></h5>
<div class="section" id="id9">
<h6><a class="toc-backref" href="#id56">Add a Compute Node</a><a class="headerlink" href="#id9" title="Permalink to this headline">¶</a></h6>
<ul class="simple">
<li>From the Undercloud, create a <cite>instackenv.json</cite> file describing the new node. Import the file using Ironic.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">source</span> ~/stackrc
$ openstack baremetal import --json instackenv.json
</pre></div>
</div>
<ul class="simple">
<li>Automatically configure it to use the existing kernel and ramdisk for PXE booting.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack baremetal configure boot
</pre></div>
</div>
<ul class="simple">
<li>Set the new node to the “manageable” state. Then introspect the new node so Ironic can automatically determine it’s resources and hardware information.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack baremetal node manage &lt;NODE_UUID&gt;
$ openstack overcloud node introspect &lt;NODE_UUID&gt; --provided
</pre></div>
</div>
<ul class="simple">
<li>Configure the node to be a compute node.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack baremetal node <span class="nb">set</span> --property <span class="nv">capabilities</span><span class="o">=</span><span class="s1">&#39;profile:compute,boot_option:local&#39;</span> &lt;NODE_UUID&gt;
</pre></div>
</div>
<ul class="simple">
<li>Redeploy the Overcloud while specifying the number of compute nodes that should exist in total after it is complete. The <cite>ComputeCount</cite> parameter in the Heat templates should also be increased to reflect it’s new value.</li>
</ul>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack overcloud deploy &lt;DEPLOY_OPTIONS&gt; --templates --compute-scale &lt;NEW_TOTAL_NUMBER_OF_ALL_COMPUTE_NODES&gt;
</pre></div>
</div>
<p>[77]</p>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="id10">
<h2><a class="toc-backref" href="#id57">Configurations</a><a class="headerlink" href="#id10" title="Permalink to this headline">¶</a></h2>
<p>This section will focus on important settings for each service’s
configuration files.</p>
<div class="section" id="common">
<h3><a class="toc-backref" href="#id58">Common</a><a class="headerlink" href="#common" title="Permalink to this headline">¶</a></h3>
<p>These are general configuration options that apply to most OpenStack
configuration files.</p>
<div class="section" id="database">
<h4><a class="toc-backref" href="#id59">Database</a><a class="headerlink" href="#database" title="Permalink to this headline">¶</a></h4>
<p>Different database servers can be used by the API services on the
controller nodes.</p>
<ul>
<li><p class="first">MariaDB/MySQL. The original “<code class="docutils literal notranslate"><span class="pre">mysql://</span></code>” connector can be used for the MySQL-Python library. Starting with Liberty, the newer PyMySQL library was added for Python 3 support. [31] CentOS first added the required <code class="docutils literal notranslate"><span class="pre">python2-PyMySQL</span></code> package to support it in the Pike release. [34][79]</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">mysql+pymysql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;MYSQL_HOST&gt;:&lt;MYSQL_PORT&gt;/&lt;DATABASE&gt;</span>
</pre></div>
</div>
</li>
<li><p class="first">PostgreSQL. Requires the “psycopg2” Python library. [32]</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">postgresql://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;POSTGRESQL_HOST&gt;:&lt;POSTGRESQL_PORT&gt;/&lt;DATABASE&gt;</span>
</pre></div>
</div>
</li>
<li><p class="first">SQLite.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">sqlite:///&lt;DATABASE&gt;.sqlite</span>
</pre></div>
</div>
</li>
<li><p class="first">MongoDB is generally only used for Ceilometer when it is not using
the Gnocchi back-end. [33]</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[database]</span>
<span class="na">mongodb://&lt;USER&gt;:&lt;PASSWORD&gt;@&lt;MONGODB_HOST&gt;:&lt;MONGODB_PORT&gt;/&lt;DATABASE&gt;</span>
</pre></div>
</div>
</li>
</ul>
</div>
<div class="section" id="messaging">
<h4><a class="toc-backref" href="#id60">Messaging</a><a class="headerlink" href="#messaging" title="Permalink to this headline">¶</a></h4>
<p>For high availability and scalability, servers should be configured with
a messaging agent. This allows a client’s request to correctly be
handled by the messaging queue and sent to one node to process that
request.</p>
<p>The configuration has been consolidated into the <code class="docutils literal notranslate"><span class="pre">transport_url</span></code>
option. Multiple messaging hosts can be defined by using a comma before
naming a virtual host.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="na">transport_url</span> <span class="o">=</span> <span class="s">&lt;TRANSPORT&gt;://&lt;USER1&gt;:&lt;PASS1&gt;@&lt;HOST1&gt;:&lt;PORT1&gt;,&lt;USER2&gt;:&lt;PASS2&gt;@&lt;HOST2&gt;:&lt;PORT2&gt;/&lt;VIRTUAL_HOST&gt;</span>
</pre></div>
</div>
<p>Scenario #1 - RabbitMQ</p>
<p>On the controller nodes, RabbitMQ needs to be installed. Then a user
must be created with full privileges.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo rabbitmqctl add_user &lt;RABBIT_USER&gt; &lt;RABBIT_PASSWORD&gt;
$ sudo rabbitmqctl set_permissions openstack <span class="s2">&quot;.*&quot;</span> <span class="s2">&quot;.*&quot;</span> <span class="s2">&quot;.*&quot;</span>
</pre></div>
</div>
<p>In the configuration file for every service, set the transport_url
options for RabbitMQ. A virtual host is not required. By default it will
use <code class="docutils literal notranslate"><span class="pre">/</span></code>.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">transport_url</span> <span class="o">=</span> <span class="s">rabbit://&lt;RABBIT_USER&gt;:&lt;RABBIT_PASSWORD&gt;@&lt;RABBIT_HOST&gt;/&lt;VIRTUAL_HOST&gt;</span>
</pre></div>
</div>
<p>[35]</p>
<p>Scenario #2 - ZeroMQ</p>
<p>This provides the best performance and stability. Scalability becomes a
concern only when getting into hundreds of nodes. Instead of relying on
a messaging queue, OpenStack services talk directly to each other using
the ZeroMQ library. Redis is required to be running and installed for
acting as a message storage back-end for all of the servers. [35][36]</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">transport_url</span> <span class="o">=</span> <span class="s">&quot;zmq+redis://&lt;REDIS_HOST&gt;:6379&quot;</span>
</pre></div>
</div>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[oslo_messaging_zmq]</span>
<span class="na">rpc_zmq_bind_address</span> <span class="o">=</span> <span class="s">&lt;IP_ADDRESS&gt;</span>
<span class="na">rpc_zmq_host</span> <span class="o">=</span> <span class="s">&lt;FQDN_OR_IP_ADDRESS&gt;</span>
</pre></div>
</div>
<p>Alternatively, for high availability, use Redis Sentinel servers in <code class="docutils literal notranslate"><span class="pre">transport_url</span></code>.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">transport_url</span> <span class="o">=</span> <span class="s">&quot;zmq+sentinel://&lt;REDIS_SENTINEL_HOST1&gt;:26379,&lt;REDI_SENTINEL_HOST2&gt;:26379&quot;</span>
</pre></div>
</div>
<p>For all-in-one deployments, the minimum requirement is to specify that ZeroMQ should be used. This will use the “MatchmakerDummy” driver that will send messages to itself.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">transport_url</span> <span class="o">=</span> <span class="s">&quot;zmq://&quot;</span>
</pre></div>
</div>
<p>[37]</p>
</div>
</div>
<div class="section" id="ironic">
<h3><a class="toc-backref" href="#id61">Ironic</a><a class="headerlink" href="#ironic" title="Permalink to this headline">¶</a></h3>
<div class="section" id="drivers">
<h4><a class="toc-backref" href="#id62">Drivers</a><a class="headerlink" href="#drivers" title="Permalink to this headline">¶</a></h4>
<p>Ironic supports different ways of managing power cycling of managed nodes. The default enabled driver is IPMITool.</p>
<p>File: /etc/ironic/ironic.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">enabled_drivers</span> <span class="o">=</span> <span class="s">&lt;DRIVER1&gt;, &lt;DRIVER2&gt;, DRIVER3&gt;</span>
</pre></div>
</div>
<p>Supported Drivers:</p>
<ul class="simple">
<li>CIMC: Cisco UCS servers (C series only).</li>
<li>iDRAC.</li>
<li>iLO: HPE ProLiant servers.</li>
<li>HP OneView.</li>
<li>IPMITool.</li>
<li>iRMC: FUJITSU PRIMERGY servers.</li>
<li>SNMP power racks.</li>
<li>UCS: Cisco UCS servers (B and C series).</li>
</ul>
<p>Each driver has different dependencies and configurations as outlined <a class="reference external" href="https://docs.openstack.org/ironic/queens/admin/drivers.html">here</a>.</p>
<p>Unsupported <a class="reference external" href="http://ironic-staging-drivers.readthedocs.io/">Ironic Staging Drivers</a>:</p>
<ul class="simple">
<li>AMT</li>
<li>iBoot</li>
<li>Wake-On-Lan</li>
</ul>
<p>Unsupported Drivers:</p>
<ul class="simple">
<li>MSFT OCS</li>
<li>SeaMicro</li>
<li>VirtualBox</li>
</ul>
<p>[75]</p>
</div>
</div>
<div class="section" id="keystone">
<h3><a class="toc-backref" href="#id63">Keystone</a><a class="headerlink" href="#keystone" title="Permalink to this headline">¶</a></h3>
<div class="section" id="api-v3">
<h4><a class="toc-backref" href="#id64">API v3</a><a class="headerlink" href="#api-v3" title="Permalink to this headline">¶</a></h4>
<p>In Mitaka, the Keystone v2.0 API has been deprecated. It will be removed entirely from OpenStack in the <code class="docutils literal notranslate"><span class="pre">T</span></code> release. [38] It is possible to run both v2.0 and v3 at the same time but it’s desirable to move towards the v3 standard. If both have to be enabled, services should be configured to use v2.0 or else problems can occur with v3’s domain scoping. For disabling v2.0 entirely, Keystone’s API paste configuration needs to have these lines removed (or commented out) and then the web server should be restarted.</p>
<p>File: /etc/keystone/keystone-paste.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[pipeline:public_api]</span>
<span class="na">pipeline</span> <span class="o">=</span> <span class="s">cors sizelimit url_normalize request_id admin_token_auth build_auth_context token_auth json_body ec2_extension public_service</span>

<span class="k">[pipeline:admin_api]</span>
<span class="na">pipeline</span> <span class="o">=</span> <span class="s">cors sizelimit url_normalize request_id admin_token_auth build_auth_context token_auth json_body ec2_extension s3_extension admin_service</span>

<span class="k">[composite:main]</span>
<span class="na">/v2.0</span> <span class="o">=</span> <span class="s">public_api</span>

<span class="k">[composite:admin]</span>
<span class="na">/v2.0</span> <span class="o">=</span> <span class="s">admin_api</span>
</pre></div>
</div>
<p>[39]</p>
</div>
<div class="section" id="token-provider">
<h4><a class="toc-backref" href="#id65">Token Provider</a><a class="headerlink" href="#token-provider" title="Permalink to this headline">¶</a></h4>
<p>The token provider is used to create and delete tokens for
authentication. Different providers can be configured.</p>
<p>File: /etc/keystone/keystone.conf</p>
<p>Scenario #1 - UUID (default)</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[token]</span>
<span class="na">provider</span> <span class="o">=</span> <span class="s">uuid</span>
</pre></div>
</div>
<p>Scenario #2 - Fernet (recommended)</p>
<p>This provides the fastest token creation and validation. A public and private key will need to be created for Fernet and the
related Credential authentication.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[token]</span>
<span class="na">provider</span> <span class="o">=</span> <span class="s">fernet</span>

<span class="k">[fernet_tokens]</span>
<span class="na">key_repository</span> <span class="o">=</span> <span class="s">/etc/keystone/fernet-keys/</span>

<span class="k">[credential]</span>
<span class="na">provider</span> <span class="o">=</span> <span class="s">fernet</span>
<span class="na">key_repository</span> <span class="o">=</span> <span class="s">/etc/keystone/credential-keys/</span>
</pre></div>
</div>
<ul>
<li><p class="first">Create the required keys:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo mkdir /etc/keystone/fernet-keys/
$ sudo chmod <span class="m">750</span> /etc/keystone/fernet-keys/
$ sudo chown keystone.keystone /etc/keystone/fernet-keys/
$ sudo keystone-manage fernet_setup --keystone-user keystone --keystone-group keystone
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo mkdir /etc/keystone/credential-keys/
$ sudo chmod <span class="m">750</span> /etc/keystone/credential-keys/
$ sudo chown keystone.keystone /etc/keystone/credential-keys/
$ sudo keystone-manage credential_setup --keystone-user keystone --keystone-group keystone
</pre></div>
</div>
</li>
</ul>
<p>[40][41]</p>
<p>Scenario #3 - PKI</p>
<p>PKI tokens have been removed since the Ocata release. [42]</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[token]</span>
<span class="na">provider</span> <span class="o">=</span> <span class="s">pki</span>
</pre></div>
</div>
<ul>
<li><p class="first">Create the certificates. A new directory “/etc/keystone/ssl/” will be used to store these files.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo keystone-manage pki_setup --keystone-user keystone --keystone-group keystone
</pre></div>
</div>
</li>
</ul>
</div>
</div>
<div class="section" id="id11">
<h3><a class="toc-backref" href="#id66">Nova</a><a class="headerlink" href="#id11" title="Permalink to this headline">¶</a></h3>
<p>File: /etc/nova/nova.conf</p>
<ul class="simple">
<li>For the controller nodes, specify the connection database connection strings for both the “nova” and “nova_api” databases.</li>
</ul>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[api_database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">&lt;DB_PROVIDER&gt;//&lt;DB_USER&gt;:&lt;DB_PASS&gt;@&lt;DB_HOST&gt;/nova_api</span>
<span class="k">[database]</span>
<span class="na">connection</span> <span class="o">=</span> <span class="s">&lt;DB_PROVIDER&gt;//&lt;DB_USER&gt;:&lt;DB_PASS&gt;@&lt;DB_HOST&gt;/nova</span>
</pre></div>
</div>
<ul class="simple">
<li>Enable support for the Nova API and Nova’s metadata API. If “metedata” is specified here, then the “openstack-nova-api” will handle the metadata and not “openstack-nova-metadata-api.”</li>
</ul>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">enabled_apis</span> <span class="o">=</span> <span class="s">osapi_compute,metadata</span>
</pre></div>
</div>
<ul class="simple">
<li>Do not inject passwords, SSH keys, or partitions via Nova. This is recommended for Ceph storage back-ends. [46] This should be handled by the Nova’s metadata service that will use cloud-init instead of Nova itself. This will either be “openstack-nova-api” or “openstack-nova-metadata-api” depending on the configuration.</li>
</ul>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[libvirt]</span>
<span class="na">inject_password</span> <span class="o">=</span> <span class="s">False</span>
<span class="na">inject_key</span> <span class="o">=</span> <span class="s">False</span>
<span class="na">inject_partition</span> <span class="o">=</span> <span class="s">-2</span>
</pre></div>
</div>
<div class="section" id="hypervisors">
<h4><a class="toc-backref" href="#id67">Hypervisors</a><a class="headerlink" href="#hypervisors" title="Permalink to this headline">¶</a></h4>
<p>Nova supports a wide range of virtualization technologies. Full hardware
virtualization, paravirtualization, or containers can be used. Even
Windows’ Hyper-V is supported.</p>
<p>File:</p>
<p>Scenario #1 - KVM</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">compute_driver</span> <span class="o">=</span> <span class="s">libvirt.LibvirtDriver</span>
<span class="k">[libvirt]</span>
<span class="na">virt_type</span> <span class="o">=</span> <span class="s">kvm</span>
</pre></div>
</div>
<p>Scenario #2 - Xen</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">compute_driver</span> <span class="o">=</span> <span class="s">libvirt.LibvirtDriver</span>
<span class="k">[libvirt]</span>
<span class="na">virt_type</span> <span class="o">=</span> <span class="s">xen</span>
</pre></div>
</div>
<p>Scenario #3 - LXC</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">compute_driver</span> <span class="o">=</span> <span class="s">libvirt.LibvirtDriver</span>
<span class="k">[libvirt]</span>
<span class="na">virt_type</span> <span class="o">=</span> <span class="s">lxc</span>
</pre></div>
</div>
<p>[43]</p>
</div>
<div class="section" id="cpu-pinning">
<h4><a class="toc-backref" href="#id68">CPU Pinning</a><a class="headerlink" href="#cpu-pinning" title="Permalink to this headline">¶</a></h4>
<ul>
<li><p class="first">Verify that the processor(s) has hardware support for non-uniform
memory access (NUMA). If it does, NUMA may still need to be turned on
in the BIOS. NUMA nodes are the physical processors. These processors
are then mapped to specific sectors of RAM.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo lscpu <span class="p">|</span> grep NUMA
NUMA node<span class="o">(</span>s<span class="o">)</span>:          <span class="m">2</span>
NUMA node0 CPU<span class="o">(</span>s<span class="o">)</span>:     <span class="m">0</span>-9,20-29
NUMA node1 CPU<span class="o">(</span>s<span class="o">)</span>:     <span class="m">10</span>-19,30-39
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo numactl --hardware
available: <span class="m">2</span> nodes <span class="o">(</span><span class="m">0</span>-1<span class="o">)</span>
node <span class="m">0</span> cpus: <span class="m">0</span> <span class="m">1</span> <span class="m">2</span> <span class="m">3</span> <span class="m">4</span> <span class="m">5</span> <span class="m">6</span> <span class="m">7</span> <span class="m">8</span> <span class="m">9</span> <span class="m">20</span> <span class="m">21</span> <span class="m">22</span> <span class="m">23</span> <span class="m">24</span> <span class="m">25</span> <span class="m">26</span> <span class="m">27</span> <span class="m">28</span> <span class="m">29</span>
node <span class="m">0</span> size: <span class="m">49046</span> MB
node <span class="m">0</span> free: <span class="m">31090</span> MB
node <span class="m">1</span> cpus: <span class="m">10</span> <span class="m">11</span> <span class="m">12</span> <span class="m">13</span> <span class="m">14</span> <span class="m">15</span> <span class="m">16</span> <span class="m">17</span> <span class="m">18</span> <span class="m">19</span> <span class="m">30</span> <span class="m">31</span> <span class="m">32</span> <span class="m">33</span> <span class="m">34</span> <span class="m">35</span> <span class="m">36</span> <span class="m">37</span> <span class="m">38</span> <span class="m">39</span>
node <span class="m">1</span> size: <span class="m">49152</span> MB
node <span class="m">1</span> free: <span class="m">31066</span> MB
node distances:
node   <span class="m">0</span>   <span class="m">1</span>
  <span class="m">0</span>:  <span class="m">10</span>  <span class="m">21</span>
  <span class="m">1</span>:  <span class="m">21</span>  <span class="m">10</span>
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo virsh nodeinfo <span class="p">|</span> grep NUMA
NUMA cell<span class="o">(</span>s<span class="o">)</span>:        <span class="m">2</span>
</pre></div>
</div>
</li>
</ul>
<p>[44]</p>
<ul class="simple">
<li>Append the NUMA filter “NUMATopologyFilter” to the Nova <code class="docutils literal notranslate"><span class="pre">scheduler_default_filters</span></code> key.</li>
</ul>
<p>File: /etc/nova/nova.conf</p>
<blockquote>
<div><div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">scheduler_default_filters</span> <span class="o">=</span> <span class="s">&lt;EXISTING_FILTERS&gt;,NUMATopologyFilter</span>
</pre></div>
</div>
</div></blockquote>
<ul>
<li><p class="first">Restart the Nova scheduler service on the controller node(s).</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo systemctl restart openstack-nova-scheduler
</pre></div>
</div>
</li>
<li><p class="first">Set the aggregate/availability zone to allow pinning.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack aggregate create &lt;AGGREGATE_ZONE&gt;
$ openstack aggregate <span class="nb">set</span> --property <span class="nv">pinned</span><span class="o">=</span><span class="nb">true</span> &lt;AGGREGATE_ZONE&gt;
</pre></div>
</div>
</li>
<li><p class="first">Add the compute hosts to the new aggregate zone.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack host list <span class="p">|</span> grep compute
$ openstack aggregate host add &lt;AGGREGATE_ZONE&gt; &lt;COMPUTE_HOST&gt;
</pre></div>
</div>
</li>
<li><p class="first">Modify a flavor to provide dedicated CPU pinning. There are three supported policies to use:</p>
<blockquote>
<div><ul>
<li><p class="first">isolate = Use cores on the same physical processor. Do not allocate any threads.</p>
</li>
<li><p class="first">prefer (default) = Cores and threads should be on the same physical processor. Fallback to using mixed cores and threads across different processors if there are not enough resources available.</p>
</li>
<li><p class="first">require = Cores and threads must be on the same physical processor.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack flavor <span class="nb">set</span> &lt;FLAVOR_ID&gt; --property hw:cpu_policy<span class="o">=</span>dedicated --property hw:cpu_thread_policy<span class="o">=</span>&lt;POLICY&gt;
</pre></div>
</div>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Alternatively, set the CPU pinning properties on an image.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack image <span class="nb">set</span> &lt;IMAGE_ID&gt; --property <span class="nv">hw_cpu_policy</span><span class="o">=</span>dedicated --property <span class="nv">hw_cpu_thread_policy</span><span class="o">=</span>&lt;POLICY&gt;
</pre></div>
</div>
</li>
</ul>
<p>[45]</p>
</div>
<div class="section" id="id12">
<h4><a class="toc-backref" href="#id69">Ceph</a><a class="headerlink" href="#id12" title="Permalink to this headline">¶</a></h4>
<p>Nova can be configured to use Ceph as the storage provider for the instance. This works with any QEMU and Libvirt based hypervisor.</p>
<p>File: /etc/nova/nova.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[libvirt]</span>
<span class="na">images_type</span> <span class="o">=</span> <span class="s">rbd</span>
<span class="na">images_rbd_pool</span> <span class="o">=</span> <span class="s">&lt;CEPH_VOLUME_POOL&gt;</span>
<span class="na">images_rbd_ceph_conf</span> <span class="o">=</span> <span class="s">/etc/ceph/ceph.conf</span>
<span class="na">rbd_user</span> <span class="o">=</span> <span class="s">&lt;CEPHX_USER&gt;</span>
<span class="na">rbd_secret_uuid</span> <span class="o">=</span> <span class="s">&lt;LIBVIRT_SECRET_UUID&gt;</span>
<span class="na">disk_cachemodes</span><span class="o">=</span><span class="s">&quot;network=writeback&quot;</span>
</pre></div>
</div>
<p>[46]</p>
</div>
<div class="section" id="nested-virtualization">
<h4><a class="toc-backref" href="#id70">Nested Virtualization</a><a class="headerlink" href="#nested-virtualization" title="Permalink to this headline">¶</a></h4>
<p>Nested virtualization allows virtual machines to run virtual machines
inside of them.</p>
<p>The kernel module must be stopped, the nested setting enabled, and then
the module must be started again.</p>
<p>Intel:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo rmmod kvm_intel
$ <span class="nb">echo</span> “options kvm_intel <span class="nv">nested</span><span class="o">=</span><span class="m">1</span>” <span class="p">|</span> sudo tee -a /etc/modprobe.d/kvm_intel.conf
$ sudo modprobe kvm_intel
</pre></div>
</div>
<p>AMD:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo rmmod kvm_amd
$ <span class="nb">echo</span> “options kvm_amd <span class="nv">nested</span><span class="o">=</span><span class="m">1</span>” <span class="p">|</span> sudo tee -a /etc/modprobe.d/kvm_amd.conf
$ sudo modprobe kvm_amd
</pre></div>
</div>
<ul class="simple">
<li>Use a hypervisor technology that supports nested virtualization such as KVM.</li>
</ul>
<p>File: /etc/nova/nova.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[libvirt]</span>
<span class="na">virt_type</span> <span class="o">=</span> <span class="s">kvm</span>
<span class="na">cpu_mode</span> <span class="o">=</span> <span class="s">host-passthrough</span>
</pre></div>
</div>
<p>[47]</p>
</div>
</div>
<div class="section" id="neutron">
<h3><a class="toc-backref" href="#id71">Neutron</a><a class="headerlink" href="#neutron" title="Permalink to this headline">¶</a></h3>
<div class="section" id="network-types">
<h4><a class="toc-backref" href="#id72">Network Types</a><a class="headerlink" href="#network-types" title="Permalink to this headline">¶</a></h4>
<p>In OpenStack, there are two common scenarios for networks: “provider”
and “self-service.”</p>
<p>Provider is is a simpler approach. It gives virtual machines direct
access to a bridge device.</p>
<p>Self-service networks are more complex due to the added bridge and
tunnel devices. This complexity allows for more advanced features such
as isolated private networks, load-balancing-as-a-service (LBaaS),
Firewall-as-a-Service (FWaaS), and more. [48]</p>
<div class="section" id="provider-networks">
<h5><a class="toc-backref" href="#id73">Provider Networks</a><a class="headerlink" href="#provider-networks" title="Permalink to this headline">¶</a></h5>
<div class="section" id="linux-bridge">
<h6><a class="toc-backref" href="#id74">Linux Bridge</a><a class="headerlink" href="#linux-bridge" title="Permalink to this headline">¶</a></h6>
<p><a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/deploy-lb-provider.html">https://docs.openstack.org/neutron/queens/admin/deploy-lb-provider.html</a></p>
</div>
<div class="section" id="open-vswitch">
<h6><a class="toc-backref" href="#id75">Open vSwitch</a><a class="headerlink" href="#open-vswitch" title="Permalink to this headline">¶</a></h6>
<p><a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/deploy-ovs-provider.html">https://docs.openstack.org/neutron/queens/admin/deploy-ovs-provider.html</a></p>
</div>
</div>
<div class="section" id="self-service-networks">
<h5><a class="toc-backref" href="#id76">Self-Service Networks</a><a class="headerlink" href="#self-service-networks" title="Permalink to this headline">¶</a></h5>
<div class="section" id="id13">
<h6><a class="toc-backref" href="#id77">Linux Bridge</a><a class="headerlink" href="#id13" title="Permalink to this headline">¶</a></h6>
<p><a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/deploy-lb-selfservice.html">https://docs.openstack.org/neutron/queens/admin/deploy-lb-selfservice.html</a></p>
</div>
<div class="section" id="id14">
<h6><a class="toc-backref" href="#id78">Open vSwitch</a><a class="headerlink" href="#id14" title="Permalink to this headline">¶</a></h6>
<p>One device is required, but it is recommended to separate traffic onto
two different network interfaces. There is <code class="docutils literal notranslate"><span class="pre">br-vlan</span></code> (sometimes also
referred to as <code class="docutils literal notranslate"><span class="pre">br-provider</span></code>) for internal tagged traffic and
<code class="docutils literal notranslate"><span class="pre">br-ex</span></code> for external connectivity.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ovs-vsctl add-br br-vlan
$ sudo ovs-vsctl add-port br-vlan &lt;VLAN_INTERFACE&gt;
$ sudo ovs-vsctl add-br br-ex
$ sudo ovs-vsctl add-port br-ex &lt;EXTERNAL_INTERFACE&gt;
</pre></div>
</div>
<p>File: /etc/neutron/neutron.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">core_plugin</span> <span class="o">=</span> <span class="s">ml2</span>
<span class="na">service_plugins</span> <span class="o">=</span> <span class="s">router</span>
<span class="na">allow_overlapping_ips</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>File: /etc/neutron/plugins/ml2/ml2_conf.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ml2]</span>
<span class="na">type_drivers</span> <span class="o">=</span> <span class="s">flat,vlan,vxlan</span>
<span class="na">tenant_network_types</span> <span class="o">=</span> <span class="s">vxlan</span>
<span class="na">mechanism_drivers</span> <span class="o">=</span> <span class="s">openvswitch,l2population</span>
<span class="k">[ml2_type_vxlan]</span>
<span class="na">vni_ranges</span> <span class="o">=</span> <span class="s">&lt;START_NUMBER&gt;,&lt;END_NUMBER&gt;</span>
</pre></div>
</div>
<ul class="simple">
<li>The <code class="docutils literal notranslate"><span class="pre">&lt;LABEL&gt;</span></code> can be any unique name. It is used as an alias for the interface name. The “local_ip” address should be accessible on the <code class="docutils literal notranslate"><span class="pre">br-vlan</span></code> interface.</li>
</ul>
<p>File: /etc/neutron/plugins/ml2/openvswitch_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ovs]</span>
<span class="na">bridge_mappings</span> <span class="o">=</span> <span class="s">&lt;LABEL&gt;:br-vlan</span>
<span class="na">local_ip</span> <span class="o">=</span> <span class="s">&lt;IP_ADDRESS&gt;</span>
<span class="k">[agent]</span>
<span class="na">tunnel_types</span> <span class="o">=</span> <span class="s">vxlan</span>
<span class="na">l2_population</span> <span class="o">=</span> <span class="s">True</span>
<span class="k">[securitygroup]</span>
<span class="na">firewall_driver</span> <span class="o">=</span> <span class="s">iptables_hybrid</span>
</pre></div>
</div>
<ul class="simple">
<li>The “external_network_bridge” key should be left defined with no value.</li>
</ul>
<p>File: /etc/neutron/l3_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">interface_driver</span> <span class="o">=</span> <span class="s">openvswitch</span>
<span class="na">external_network_bridge</span> <span class="o">=</span>
</pre></div>
</div>
<p>[49]</p>
<p>On the controller node, restart the Nova API service and then start the
required Neutron services.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo systemctl restart openstack-nova-api
$ sudo systemctl <span class="nb">enable</span> neutron-server neutron-openvswitch-agent neutron-dhcp-agent neutron-metadata-agent neutron-l3-agent
$ sudo systemctl start neutron-server neutron-openvswitch-agent neutron-dhcp-agent neutron-metadata-agent neutron-l3-agent
</pre></div>
</div>
<p>Finally, on the compute nodes, restart the compute service and then
start the Open vSwitch agent.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo systemctl restart openstack-nova-compute
$ sudo systemctl <span class="nb">enable</span> neutron-openvswitch-agent
$ sudo systemctl start neutron-openvswitch-agent
</pre></div>
</div>
<p>[50]</p>
</div>
</div>
</div>
<div class="section" id="dns">
<h4><a class="toc-backref" href="#id79">DNS</a><a class="headerlink" href="#dns" title="Permalink to this headline">¶</a></h4>
<p>By default, Neutron does not provide any DNS resolvers. This means that
DNS will not work. It is possible to either provide a default list of
name servers or configure Neutron to refer to the relevant
/etc/resolv.conf file on the server.</p>
<p>File: /etc/neutron/dhcp_agent.ini</p>
<p>Scenario #1 - Define a list of default resolvers (recommended)</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">dnsmasq_dns_servers</span> <span class="o">=</span> <span class="s">8.8.8.8,8.8.4.4</span>
</pre></div>
</div>
<p>Scenario #2 - Leave resolvers to be configured by the subnet details</p>
<ul class="simple">
<li>Nothing needs to be configured. This is the default setting.</li>
</ul>
<p>Scenario #3 - Do not provide resolvers, use the ones provided in the image</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">dnsmasq_local_resolv</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>[51]</p>
</div>
<div class="section" id="metadata">
<h4><a class="toc-backref" href="#id80">Metadata</a><a class="headerlink" href="#metadata" title="Permalink to this headline">¶</a></h4>
<p>The metadata service provides useful information about the instance from
the IP address 169.254.169.254/32. This service is also used to
communicate with “cloud-init” on the instance to configure SSH keys and
other post-boot tasks.</p>
<p>Assuming authentication is already configured, set these options for the
OpenStack environment. These are the basics needed before the metadata
service can be used correctly. Then it can also be configured to use DHCP
namespaces (layer 2) or router namespaces (layer 3) for
delivering/receiving requests.</p>
<p>File: /etc/neutron/metadata_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">nova_metadata_ip</span> <span class="o">=</span> <span class="s">&lt;CONTROLLER_IP&gt;</span>
<span class="na">metadata_proxy_shared_secret</span> <span class="o">=</span> <span class="s">&lt;SECRET_KEY&gt;</span>
</pre></div>
</div>
<p>File: /etc/nova/nova.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">enabled\_apis</span> <span class="o">=</span> <span class="s">osapi\_compute,metadata</span>
<span class="k">[neutron]</span>
<span class="na">service_metadata_proxy</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">metadata_proxy_shared_secret</span> <span class="o">=</span> <span class="s">&lt;SECRET_KEY&gt;</span>
</pre></div>
</div>
<p>Scenario #1 - DHCP Namespace (recommended for DVR)</p>
<p>File: /etc/neutron/dhcp_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">force_metadata</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">enable_isolated_metadata</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">enable_metadata_network</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>File: /etc/neutron/l3_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">enable_metadata_proxy</span> <span class="o">=</span> <span class="s">False</span>
</pre></div>
</div>
<p>Scenario #2 - Router Namespace</p>
<p>File: /etc/neutron/dhcp_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">force_metadata</span> <span class="o">=</span> <span class="s">False</span>
<span class="na">enable_isolated_metadata</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">enable_metadata_network</span> <span class="o">=</span> <span class="s">False</span>
</pre></div>
</div>
<p>File: /etc/neutron/l3_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">enable_metadata_proxy</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>[52]</p>
</div>
<div class="section" id="load-balancing-as-a-service">
<h4><a class="toc-backref" href="#id81">Load-Balancing-as-a-Service</a><a class="headerlink" href="#load-balancing-as-a-service" title="Permalink to this headline">¶</a></h4>
<p>Load-Balancing-as-a-Service version 2 (LBaaS v2) has been stable since
Liberty. It can be configured with either the HAProxy or Octavia
back-end. LBaaS v1 has been removed since the Newton release.</p>
<ul class="simple">
<li>Append the LBaaSv2 service plugin.</li>
</ul>
<p>File: /etc/neutron/neutron.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">service_plugins</span> <span class="o">=</span> <span class="s">&lt;EXISTING_PLUGINS&gt;,neutron_lbaas.services.loadbalancer.plugin.LoadBalancerPluginv2</span>
</pre></div>
</div>
<ul class="simple">
<li>Specify the <code class="docutils literal notranslate"><span class="pre">&lt;INTERFACE_DRIVER&gt;</span></code> as either <code class="docutils literal notranslate"><span class="pre">linuxbridge</span></code> or <code class="docutils literal notranslate"><span class="pre">openvswitch</span></code>.</li>
</ul>
<p>File: /etc/neutron/lbaas_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">interface_driver</span> <span class="o">=</span> <span class="s">&lt;INTERFACE_DRIVER&gt;</span>
</pre></div>
</div>
<p>Scenario #1 - HAProxy</p>
<p>File: /etc/neutron/neutron_lbaas.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[service_providers]</span>
<span class="na">service_provider</span> <span class="o">=</span> <span class="s">LOADBALANCERV2:Haproxy:neutron_lbaas.drivers.haproxy.plugin_driver.HaproxyOnHostPluginDriver:default</span>
</pre></div>
</div>
<ul class="simple">
<li>Specify the HAProxy driver and the group that HAProxy runs as. In RHEL, it is <code class="docutils literal notranslate"><span class="pre">haproxy</span></code>.</li>
</ul>
<p>File: /etc/neutron/lbaas_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">device_driver</span> <span class="o">=</span> <span class="s">neutron_lbaas.drivers.haproxy.namespace_driver.HaproxyNSDriver</span>
<span class="k">[haproxy]</span>
<span class="na">user_group</span> <span class="o">=</span> <span class="s">haproxy</span>
</pre></div>
</div>
<p>Scenario #2 - Octavia</p>
<p>File: /etc/neutron/neutron_lbaas.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[service_providers]</span>
<span class="na">service_provider</span> <span class="o">=</span> <span class="s">LOADBALANCERV2:Octavia:neutron_lbaas.drivers.octavia.driver.OctaviaDriver:default</span>
</pre></div>
</div>
<p>[53]</p>
</div>
<div class="section" id="quality-of-service">
<h4><a class="toc-backref" href="#id82">Quality of Service</a><a class="headerlink" href="#quality-of-service" title="Permalink to this headline">¶</a></h4>
<p>The Quality of Service (QoS) plugin can be used to rate limit the amount
of bandwidth that is allowed through a network port.</p>
<ul class="simple">
<li>Append the QoS plugin to the list of service_plugins.</li>
</ul>
<p>File: /etc/neutron/neutron.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">service_plugins</span> <span class="o">=</span> <span class="s">&lt;EXISTING_PLGUINS&gt;,neutron.services.qos.qos_plugin.QoSPlugin</span>
</pre></div>
</div>
<p>Layer 2 QoS</p>
<ul class="simple">
<li>Append the QoS driver to the modular layer 2 (ML2) extension drivers.</li>
</ul>
<p>File: /etc/neutron/plugins/ml2/ml2_conf.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ml2]</span>
<span class="na">extension_drivers</span> <span class="o">=</span> <span class="s">qos</span>
</pre></div>
</div>
<ul class="simple">
<li>Also append the QoS extension directly to the modular layer 2 configuration. The three supported agents for QoS are: Linux Bridge, Open vSwitch, and SR-IOV.</li>
</ul>
<p>File: /etc/neutron/plugins/ml2/&lt;AGENT&gt;_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[agent]</span>
<span class="na">extensions</span> <span class="o">=</span> <span class="s">&lt;EXISTING_EXTENSIONS&gt;,qos</span>
</pre></div>
</div>
<p>Layer 3 QoS</p>
<ul class="simple">
<li>Append the “fip_qos” extension in the neutron-l3-agent’s configuration file.</li>
</ul>
<p>File: /etc/neutron/l3_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[agent]</span>
<span class="na">extensions</span> <span class="o">=</span> <span class="s">&lt;EXISTING_EXTENSIONS&gt;,fip_qos</span>
</pre></div>
</div>
<ul class="simple">
<li>For Open vSwitch only, this workaround is required to limit the bandwidth usage on routers.</li>
</ul>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">ovs_use_veth</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>[54]</p>
</div>
<div class="section" id="distributed-virtual-routing">
<h4><a class="toc-backref" href="#id83">Distributed Virtual Routing</a><a class="headerlink" href="#distributed-virtual-routing" title="Permalink to this headline">¶</a></h4>
<p>Distributed virtual routing (DVR) is a concept that involves deploying
routers to both the compute and network nodes to spread out resource
usage. All layer 2 traffic will be equally spread out among the servers.
Public floating IPs will still need to go through the SNAT process via
the routers on the controller or network nodes. This is only supported when the Open
vSwitch agent is used. [55]</p>
<p>File: /etc/neutron/neutron.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">router_distributed</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>File (compute node):  /etc/neutron/l3_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">agent_mode</span> <span class="o">=</span> <span class="s">dvr</span>
</pre></div>
</div>
<p>File (network node): /etc/neutron/l3_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">agent_mode</span> <span class="o">=</span> <span class="s">dvr_snat</span>
</pre></div>
</div>
<p>File: /etc/neutron/plugins/ml2/ml2_conf.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[ml2]</span>
<span class="na">mechanism_drivers</span> <span class="o">=</span> <span class="s">openvswitch,l2population</span>
</pre></div>
</div>
<p>File: /etc/neutron/plugins/ml2/openvswitch_agent.ini</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[agent]</span>
<span class="na">l2_population</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">enable_distributed_routing</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>[56]</p>
</div>
<div class="section" id="high-availability">
<h4><a class="toc-backref" href="#id84">High Availability</a><a class="headerlink" href="#high-availability" title="Permalink to this headline">¶</a></h4>
<p>High availability (HA) in Neutron allows for routers to fail-over to
another duplicate router if one fails or is no longer present. All new
routers will be highly available.</p>
<p>File: /etc/neutron/neutron.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">l3_ha</span> <span class="o">=</span> <span class="s">True</span>
<span class="na">max_l3_agents_per_router</span> <span class="o">=</span> <span class="s">2</span>
<span class="na">allow_automatic_l3agent_failover</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<p>[55]</p>
</div>
</div>
<div class="section" id="id15">
<h3><a class="toc-backref" href="#id85">Ceph</a><a class="headerlink" href="#id15" title="Permalink to this headline">¶</a></h3>
<p>For Cinder and/or Glance to work with Ceph, the Ceph configuration needs
to exist on each controller and compute node. This can be copied over
from the Ceph nodes. An example is provided below.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[global]</span>
<span class="na">fsid</span> <span class="o">=</span> <span class="s">&lt;UNIQUE_ID&gt;</span>
<span class="na">mon_initial_members</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR1_HOSTNAME&gt;</span>
<span class="na">mon_host</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR1_IP_ADDRESS&gt;</span>
<span class="na">auth_cluster_required</span> <span class="o">=</span> <span class="s">cephx</span>
<span class="na">auth_service_required</span> <span class="o">=</span> <span class="s">cephx</span>
<span class="na">auth_client_required</span> <span class="o">=</span> <span class="s">cephx</span>
<span class="na">osd_pool_default_size</span> <span class="o">=</span> <span class="s">2</span>
<span class="na">public_network</span> <span class="o">=</span> <span class="s">&lt;CEPH_NETWORK_CIDR&gt;</span>

<span class="k">[mon]</span>
<span class="na">mon_host</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR1_HOSTNAME&gt;, &lt;CEPH_MONITOR2_HOSTNAME&gt;, &lt;CEPH_MONITOR3_HOSTNAME&gt;</span>
<span class="na">mon_addr</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR1_IP_ADDRESS&gt;:6789, &lt;CEPH_MONITOR2_IP_ADDRESS&gt;:6789, &lt;CEPH_MONITOR3_IP_ADDRESS&gt;:6789</span>

<span class="k">[mon.a]</span>
<span class="na">host</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR1_HOSTNAME&gt;</span>
<span class="na">mon_addr</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR1_IP_ADDRESS&gt;:6789</span>

<span class="k">[mon.b]</span>
<span class="na">host</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR2_HOSTNAME&gt;</span>
<span class="na">mon_addr</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR2_IP_ADDRESS&gt;:6789</span>

<span class="k">[mon.c]</span>
<span class="na">host</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR3_HOSTNAME&gt;</span>
<span class="na">mon_addr</span> <span class="o">=</span> <span class="s">&lt;CEPH_MONITOR3_IP_ADDRESS&gt;:6789</span>
</pre></div>
</div>
<p>It is recommended to create a separate pool and related user for both
the Glance and Cinder service.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ceph osd pool create glance &lt;PG_NUM&gt; &lt;PGP_NUM&gt;
$ sudo ceph osd pool create cinder &lt;PG_NUM&gt; &lt;PGP_NUM&gt;
$ sudo ceph auth get-or-create client.cinder mon <span class="s1">&#39;allow r&#39;</span> osd <span class="s1">&#39;allow class-read object_prefix rbd_children, allow rwx pool=volumes&#39;</span>
$ sudo ceph auth get-or-create client.glance mon <span class="s1">&#39;allow r&#39;</span> osd <span class="s1">&#39;allow class-read object_prefix rbd_children, allow rwx pool=images&#39;</span>
</pre></div>
</div>
<p>If Cephx is turned on to utilize authentication, then a client keyring
file should be created on the controller and compute nodes. This will
allow the services to communicate to Ceph as a specific user. The
usernames should match the client users that were just created. [57]</p>
<p>File: <code class="docutils literal notranslate"><span class="pre">/etc/ceph/ceph.client.&lt;USERNAME&gt;.keyring</span></code></p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[client.&lt;USERNAME&gt;]</span>
        <span class="na">key</span> <span class="o">=</span> <span class="s">&lt;KEY&gt;</span>
</pre></div>
</div>
<p>On the controller and compute nodes the Nova, Cinder, and Glance
services require permission to read the <code class="docutils literal notranslate"><span class="pre">/etc/ceph/ceph.conf</span></code> and
client configurations at <code class="docutils literal notranslate"><span class="pre">/etc/ceph/ceph.client.&lt;USERNAME&gt;.keyring</span></code>.
The service users should be added to a common group to help securely
share these settings.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo <span class="k">for</span> openstack_service in <span class="s2">&quot;cinder glance nova&quot;</span><span class="p">;</span> <span class="k">do</span> usermod -a -G ceph <span class="si">${</span><span class="nv">openstack_service</span><span class="si">}</span><span class="p">;</span> <span class="k">done</span>
$ sudo chmod -R <span class="m">640</span> /etc/ceph/
$ sudo chown -R ceph.ceph /etc/ceph/
</pre></div>
</div>
<p>For the services to work, the relevant Python libraries for accessing
Ceph need to be installed. These can be installed by the operating
system’s package manager. [57]</p>
<p>Fedora:</p>
<ul class="simple">
<li>python-ceph-compat</li>
<li>python-rbd</li>
</ul>
<p>Debian:</p>
<ul class="simple">
<li>python-ceph</li>
</ul>
</div>
<div class="section" id="cinder">
<h3><a class="toc-backref" href="#id86">Cinder</a><a class="headerlink" href="#cinder" title="Permalink to this headline">¶</a></h3>
<p>The Cinder service provides block devices for instances.</p>
<div class="section" id="id16">
<h4><a class="toc-backref" href="#id87">Ceph</a><a class="headerlink" href="#id16" title="Permalink to this headline">¶</a></h4>
<p>Ceph has become the most popular back-end to Cinder due to it’s high
availability and scalability.</p>
<ul class="simple">
<li>Create a new <code class="docutils literal notranslate"><span class="pre">[ceph]</span></code> section for the back-end configuration. The name of this new section must reflect what is set in “enabled_backends.”</li>
</ul>
<p>File: /etc/cinder/cinder.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">enabled_backends</span> <span class="o">=</span> <span class="s">ceph</span>
<span class="na">volume_backend_name</span> <span class="o">=</span> <span class="s">volumes</span>
<span class="na">rados_connect_timeout</span> <span class="o">=</span> <span class="s">-1</span>
<span class="k">[ceph]</span>
<span class="na">volume_driver</span> <span class="o">=</span> <span class="s">cinder.volume.drivers.rbd.RBDDriver</span>
<span class="na">rbd_pool</span> <span class="o">=</span> <span class="s">&lt;RBD_VOLUME_POOL&gt;</span>
<span class="na">rbd_ceph_conf</span> <span class="o">=</span> <span class="s">/etc/ceph/ceph.conf</span>
<span class="c1">#Ceph supports efficient thin provisioned snapshots when this is set to &quot;False.&quot;</span>
<span class="na">rbd_flatten_volume_from_snapshot</span> <span class="o">=</span> <span class="s">False</span>
<span class="c1">#Only clone an image up to 5 times before creating a new copy of the image.</span>
<span class="na">rbd_max_clone_depth</span> <span class="o">=</span> <span class="s">5</span>
<span class="na">rbd_store_chunk_size</span> <span class="o">=</span> <span class="s">4</span>
<span class="c1">#Do not timeout when trying to connect to RADOS.</span>
<span class="na">rados_connect_timeout</span> <span class="o">=</span> <span class="s">-1</span>
<span class="na">glance_api_version</span> <span class="o">=</span> <span class="s">2</span>
</pre></div>
</div>
<p>File: /etc/nova/nova.conf</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[libvirt]</span>
<span class="na">images_type</span> <span class="o">=</span> <span class="s">rbd</span>
<span class="na">images_rbd_pool</span> <span class="o">=</span> <span class="s">&lt;RBD_VOLUME_POOL&gt;</span>
<span class="na">images_rbd_ceph_conf</span> <span class="o">=</span> <span class="s">/etc/ceph/ceph.conf</span>
<span class="na">rbd_user</span> <span class="o">=</span> <span class="s">&lt;CEPHX_USER&gt;</span>
<span class="c1">#This is the Libvirt secret UUID used for Cephx authentication.</span>
<span class="na">rbd_secret_uuid</span> <span class="o">=</span> <span class="s">&lt;LIBVIRT_SECRET_UUID&gt;</span>
</pre></div>
</div>
<p>[57]</p>
</div>
<div class="section" id="encryption">
<h4><a class="toc-backref" href="#id88">Encryption</a><a class="headerlink" href="#encryption" title="Permalink to this headline">¶</a></h4>
<p>Cinder volumes support the Linux LUKS encryption. The only requirement
is that the compute nodes have the “cryptsetup” package installed. [58]</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack volume <span class="nb">type</span> create LUKS
$ cinder encryption-type-create --cipher aes-xts-plain64 --key_size <span class="m">512</span> --control_location front-end LUKS nova.volume.encryptors.luks.LuksEncryptor
</pre></div>
</div>
<p>Encrypted volumes can now be created.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack volume create --size &lt;SIZE_IN_GB&gt; --type LUKS &lt;VOLUME_NAME&gt;
</pre></div>
</div>
</div>
</div>
<div class="section" id="glance">
<h3><a class="toc-backref" href="#id89">Glance</a><a class="headerlink" href="#glance" title="Permalink to this headline">¶</a></h3>
<p>Glance is used to store and manage images for instance deployment.</p>
<div class="section" id="id17">
<h4><a class="toc-backref" href="#id90">Ceph</a><a class="headerlink" href="#id17" title="Permalink to this headline">¶</a></h4>
<p>Ceph can be used to store images.</p>
<p>File: /etc/glance/glance-api.conf</p>
<ul class="simple">
<li>First configure “show_image_direct_url” to allow copy-on-write (CoW) operations for efficient usage of storage for instances. Instead of cloning the entire image, CoW will be used to store changes between the instance and the original image. This assumes that Cinder is also configured to use Ceph.</li>
<li>The back-end Ceph IP addressing will be viewable by the public Glance API. For security purposes, ensure that Ceph is not publicly accessible.</li>
</ul>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[DEFAULT]</span>
<span class="na">show_image_direct_url</span> <span class="o">=</span> <span class="s">True</span>
</pre></div>
</div>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[glance_store]</span>
<span class="na">stores</span> <span class="o">=</span> <span class="s">rbd</span>
<span class="na">default_store</span> <span class="o">=</span> <span class="s">rbd</span>
<span class="na">rbd_store_pool</span> <span class="o">=</span> <span class="s">&lt;RBD_IMAGES_POOL&gt;</span>
<span class="na">rbd_store_user</span> <span class="o">=</span> <span class="s">&lt;CEPHX_USER&gt;</span>
<span class="na">rbd_store_ceph_conf</span> <span class="o">=</span> <span class="s">/etc/ceph/ceph.conf</span>
<span class="na">rbd_store_chunk_size</span> <span class="o">=</span> <span class="s">8</span>
</pre></div>
</div>
<p>[59][80]</p>
</div>
</div>
</div>
<div class="section" id="neutron-troubleshooting">
<h2><a class="toc-backref" href="#id91">Neutron Troubleshooting</a><a class="headerlink" href="#neutron-troubleshooting" title="Permalink to this headline">¶</a></h2>
<p>Neutron is one of the most complicated services offered by OpenStack.
Due to it’s wide range of configurations and technologies that it
handles, it can be difficult to troubleshoot problems. This section aims
to clearly layout common techniques to track down and fix issues with
Neutron.</p>
<div class="section" id="id18">
<h3><a class="toc-backref" href="#id92">Open vSwitch</a><a class="headerlink" href="#id18" title="Permalink to this headline">¶</a></h3>
<div class="section" id="floating-ips">
<h4><a class="toc-backref" href="#id93">Floating IPs</a><a class="headerlink" href="#floating-ips" title="Permalink to this headline">¶</a></h4>
<p>Floating IPs can be manually added to the namespace. Depending on the
environment, these rules either need to be added to the
<code class="docutils literal notranslate"><span class="pre">snat-&lt;ROUTER_ID&gt;</span></code> namespace if it exists or the
<code class="docutils literal notranslate"><span class="pre">qrouter-&lt;ROUTER_ID&gt;</span></code> namespace. All floating IPs need to be added
with the /32 CIDR, not the CIDR that represents it’s true subnet mask.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ip netns <span class="nb">exec</span> snat-&lt;ROUTER_ID&gt; iptables -t nat -A neutron-l3-agent-OUTPUT -d &lt;FLOATING_IP&gt;/32 -j DNAT --to-destination &lt;LOCAL_IP&gt;
$ sudo ip netns <span class="nb">exec</span> snat-&lt;ROUTER_ID&gt; iptables -t nat -A neutron-l3-agent-PREROUTING -d &lt;FLOATING_IP&gt;/32 -j DNAT --to-destination &lt;LOCAL_IP&gt;
$ sudo ip netns <span class="nb">exec</span> snat-&lt;ROUTER_ID&gt; iptables -t nat -A neutron-l3-agent-float-snat -s &lt;LOCAL_IP&gt;/32 -j SNAT --to-source &lt;FLOATING_IP&gt;
$ sudo ip netns <span class="nb">exec</span> snat-&lt;ROUTER_ID&gt; ip address add &lt;FLOATING_IP&gt;/32 brd &lt;FLOATING_IP&gt; dev qg-b2e3c286-b2
</pre></div>
</div>
<p>With no floating IPs allocated, the iptables NAT table in the SNAT
namespace should look similar to this.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ sudo ip netns <span class="nb">exec</span> snat-&lt;ROUTER_ID&gt; iptables -t nat -S
-P PREROUTING ACCEPT
-P INPUT ACCEPT
-P OUTPUT ACCEPT
-P POSTROUTING ACCEPT
-N neutron-l3-agent-OUTPUT
-N neutron-l3-agent-POSTROUTING
-N neutron-l3-agent-PREROUTING
-N neutron-l3-agent-float-snat
-N neutron-l3-agent-snat
-N neutron-postrouting-bottom
-A PREROUTING -j neutron-l3-agent-PREROUTING
-A OUTPUT -j neutron-l3-agent-OUTPUT
-A POSTROUTING -j neutron-l3-agent-POSTROUTING
-A POSTROUTING -j neutron-postrouting-bottom
-A neutron-l3-agent-POSTROUTING ! -i qg-&lt;NIC_ID&gt; ! -o qg-&lt;NIC_ID&gt; -m conntrack ! --ctstate DNAT -j ACCEPT
-A neutron-l3-agent-snat -o qg-&lt;NIC_ID&gt; -j SNAT --to-source &lt;PUBLIC_ROUTER_IP&gt;
-A neutron-l3-agent-snat -m mark ! --mark 0x2/0xffff -m conntrack --ctstate DNAT -j SNAT --to-source &lt;PUBLIC_ROUTER_IP&gt;
-A neutron-postrouting-bottom -m comment --comment <span class="s2">&quot;Perform source NAT on outgoing traffic.&quot;</span> -j neutron-l3-agent-snat
</pre></div>
</div>
<p>[60][61]</p>
</div>
</div>
</div>
<div class="section" id="id19">
<h2><a class="toc-backref" href="#id94">Upgrades</a><a class="headerlink" href="#id19" title="Permalink to this headline">¶</a></h2>
<p>Upgrading a production OpenStack environment requires a lot of planning.
It is recommended to test an upgrade of the environment virtually before
rolling it out to production. Automation tools generally have their own
guides but most of these guidelines should still apply to manual
deployment upgrades. The entire steps include to:</p>
<ul class="simple">
<li>Backup configuration files and databases.</li>
<li>Review the release notes of the OpenStack services that will be
upgraded. These will contain details of deprecations and new
configuration changes. <a class="reference external" href="https://releases.openstack.org/">https://releases.openstack.org/</a></li>
<li>Update configuration files. Sample configurations can be found at
<code class="docutils literal notranslate"><span class="pre">http://docs.openstack.org/&lt;RELEASE&gt;/config-reference/</span></code>.</li>
<li>If not already, consider using an automation tool such as Ansible to
deploy new service configurations.</li>
<li>Remove the old package repository for OpenStack.</li>
<li>Add the new package repository for OpenStack.</li>
<li>Update all of the packages.</li>
<li>Restart the services. <code class="docutils literal notranslate"><span class="pre">openstack-service</span> <span class="pre">restart</span></code></li>
</ul>
<p>[62]</p>
</div>
<div class="section" id="command-line-interface-utilities">
<h2><a class="toc-backref" href="#id95">Command Line Interface Utilities</a><a class="headerlink" href="#command-line-interface-utilities" title="Permalink to this headline">¶</a></h2>
<p>The OpenStack command line interface (CLI) resources used to be handled
by separate commands. These have all been modified and are managed by
the universal “openstack” command. The various options and arguments are
explained in Root Pages’ OpenStack section <a class="reference external" href="https://raw.githubusercontent.com/ekultails/rootpages/master/linux_commands.xlsx">Linux Commands excel
sheet</a>.</p>
<p>For the CLI utilities to work, the environment variables need to be set
for the project and user. This way the commands can automatically
authenticate.</p>
<ul>
<li><p class="first">Add the credentials to a text file. Use the “.sh” extension to signify it’s a shell script. A few default variables are filled in below.</p>
</li>
<li><p class="first">Keystone v2.0</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1">#unset any variables used</span>
<span class="nb">unset</span> OS_PROJECT_ID
<span class="nb">unset</span> OS_PROJECT_NAME
<span class="nb">unset</span> OS_PROJECT_DOMAIN_ID
<span class="nb">unset</span> OS_PROJECT_DOMAIN_NAME
<span class="nb">unset</span> OS_USER_ID
<span class="nb">unset</span> OS_USER_NAME
<span class="nb">unset</span> OS_USER_DOMAIN_ID
<span class="nb">unset</span> OS_USER_DOMAIN_NAME
<span class="nb">unset</span> OS_REGION_ID
<span class="nb">unset</span> OS_REGION_NAME
<span class="c1">#fill in the project, user, and endpoint details</span>
<span class="nb">export</span> <span class="nv">PROJECT_ID</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">PROJECT_NAME</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_USERNAME</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_PASSWORD</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_REGION_NAME</span><span class="o">=</span><span class="s2">&quot;RegionOne&quot;</span>
<span class="nb">export</span> <span class="nv">OS_AUTH_URL</span><span class="o">=</span><span class="s2">&quot;http://controller1:5000/v2.0&quot;</span>
<span class="nb">export</span> <span class="nv">OS_AUTH_VERSION</span><span class="o">=</span><span class="s2">&quot;2.0&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Keystone v3</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="c1">#unset any variables used</span>
<span class="nb">unset</span> OS_PROJECT_ID
<span class="nb">unset</span> OS_PROJECT_NAME
<span class="nb">unset</span> OS_PROJECT_DOMAIN_ID
<span class="nb">unset</span> OS_PROJECT_DOMAIN_NAME
<span class="nb">unset</span> OS_USER_ID
<span class="nb">unset</span> OS_USER_NAME
<span class="nb">unset</span> OS_USER_DOMAIN_ID
<span class="nb">unset</span> OS_USER_DOMAIN_NAME
<span class="nb">unset</span> OS_REGION_ID
<span class="nb">unset</span> OS_REGION_NAME
<span class="c1">#fill in the project, user, and endpoint details</span>
<span class="nb">export</span> <span class="nv">OS_PROJECT_ID</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_PROJECT_NAME</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_PROJECT_DOMAIN_NAME</span><span class="o">=</span><span class="s2">&quot;default&quot;</span>
<span class="nb">export</span> <span class="nv">OS_USERID</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_USERNAME</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_PASSWORD</span><span class="o">=</span>
<span class="nb">export</span> <span class="nv">OS_USER_DOMAIN_NAME</span><span class="o">=</span><span class="s2">&quot;default&quot;</span>
<span class="nb">export</span> <span class="nv">OS_REGION_NAME</span><span class="o">=</span><span class="s2">&quot;RegionOne&quot;</span>
<span class="nb">export</span> <span class="nv">OS_AUTH_URL</span><span class="o">=</span><span class="s2">&quot;http://controller1:5000/v3&quot;</span>
<span class="nb">export</span> <span class="nv">OS_AUTH_VERSION</span><span class="o">=</span><span class="s2">&quot;3&quot;</span>
</pre></div>
</div>
</li>
<li><p class="first">Source the credential file to load it into the shell environment:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ <span class="nb">source</span> &lt;USER_CREDENTIALS_FILE&gt;.sh
</pre></div>
</div>
</li>
<li><p class="first">View the available command line options.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack <span class="nb">help</span>
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ openstack <span class="nb">help</span> &lt;OPTION&gt;
</pre></div>
</div>
</li>
</ul>
<p>[63]</p>
</div>
<div class="section" id="orchestration">
<h2><a class="toc-backref" href="#id96">Orchestration</a><a class="headerlink" href="#orchestration" title="Permalink to this headline">¶</a></h2>
<p>Automating resource management can be accomplished in a few ways.
OpenStack provides Orchestration as a Service (OaaS) via Heat. It is
also possible to use Ansible or Vagrant to automate creating, reading,
updating, and deleting resources in an OpenStack cloud.</p>
<div class="section" id="heat">
<h3><a class="toc-backref" href="#id97">Heat</a><a class="headerlink" href="#heat" title="Permalink to this headline">¶</a></h3>
<p>Heat is used to orchestrate the deployment of multiple OpenStack
components at once. It can also install and configure software on newly
built instances.</p>
<div class="section" id="resources">
<h4><a class="toc-backref" href="#id98">Resources</a><a class="headerlink" href="#resources" title="Permalink to this headline">¶</a></h4>
<p>Heat templates use YAML formatting and are made of multiple resources.
All of the different resource types are listed here:
<a class="reference external" href="https://docs.openstack.org/heat/latest/template_guide/openstack.html">https://docs.openstack.org/heat/latest/template_guide/openstack.html</a>.
Resources use properties to create a component. If no name is specified
(for example, a network name), a random string will be used. Most
properties also accept either an exact ID of a resource or a reference
to a dynamically generated resource (which will provide it’s ID once it
has been created). [64]</p>
<p>All Heat templates must began with defining the version of OpenStack is
was designed for (using the release date as the version) and enclose all
resources in a “resources” dictionary. The version indicates that all
features up until that specific release are used. This is for backwards
compatibility reasons.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nn">---</span>
<span class="l l-Scalar l-Scalar-Plain">heat_template_version</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">2017-02-24</span>

<span class="l l-Scalar l-Scalar-Plain">resources</span><span class="p p-Indicator">:</span>
</pre></div>
</div>
<p>Valid Heat template versions include [65]:</p>
<ul class="simple">
<li>2018-03-02 (Queens)</li>
<li>2017-09-01 (Pike)</li>
<li>2017-02-24 (Ocata)</li>
<li>2016-10-14 (Newton)</li>
<li>2016-04-08 (Mitaka)</li>
<li>2015-10-15 (Liberty)</li>
<li>2015-04-30 (Kilo)</li>
<li>2014-10-16 (Juno)</li>
<li>2013-05-23 (Icehouse)</li>
</ul>
<p>This section will go over examples of the more common modules. Each
resource must be nested under the single “resources” section.</p>
<p>Syntax:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">&lt;DESCRIPTIVE_OBJECT_NAME&gt;</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;HEAT_RESOURCE_TYPE&gt;</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">&lt;PROPERTY_1&gt;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;VALUE_1&gt;</span>
    <span class="l l-Scalar l-Scalar-Plain">&lt;PROPERTY_2&gt;</span><span class="p p-Indicator">:</span>
      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">&lt;LIST_VALUE_1&gt;</span>
      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">&lt;LIST_VALUE_2&gt;</span>
    <span class="l l-Scalar l-Scalar-Plain">&lt;PROPERTY_3&gt;</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">&lt;DICTIONARY_KEY_1&gt;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;DICTIONARY_VALUE_1&gt;</span>
      <span class="l l-Scalar l-Scalar-Plain">&lt;DICTIONARY_KEY_2&gt;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;DICTIONARY_VALUE_2&gt;</span>
</pre></div>
</div>
<p>For referencing created resources (for example, creating a subnet in a
created network) the “get_resource” function should be used.</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">&lt;OBJECT_NAME&gt;</span> <span class="p p-Indicator">}</span>
</pre></div>
</div>
<p>Official examples of Heat templates can be found here:
<a class="reference external" href="https://github.com/openstack/heat-templates/tree/master/hot">https://github.com/openstack/heat-templates/tree/master/hot</a>. Below is a
demonstration on how to create a virtual machine with public networking.</p>
<ul class="simple">
<li>Create a network, assigned to the “internal_network” object.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">internal_network</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Neutron::Net</span>
</pre></div>
</div>
<ul class="simple">
<li>Create a subnet for the created network. Required properties: network
name or ID.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">internal_subnet</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Neutron::Subnet</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">network</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">internal_network</span> <span class="p p-Indicator">}</span>
    <span class="l l-Scalar l-Scalar-Plain">cidr</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">10.0.0.0/24</span>
    <span class="l l-Scalar l-Scalar-Plain">dns_nameservers</span><span class="p p-Indicator">:</span>
      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">8.8.4.4</span>
      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">8.8.8.8</span>
</pre></div>
</div>
<ul class="simple">
<li>Create a port. This object can be used during the instance creation.
Required properties: network name or ID.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">subnet_port</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Neutron::Port</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">network</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">internal_network</span> <span class="p p-Indicator">}</span>
    <span class="l l-Scalar l-Scalar-Plain">fixed_ips</span><span class="p p-Indicator">:</span>
      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">subnet_id</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">internal_subnet</span> <span class="p p-Indicator">}</span>
    <span class="l l-Scalar l-Scalar-Plain">security_groups</span><span class="p p-Indicator">:</span>
      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">basic_allow</span>
</pre></div>
</div>
<ul class="simple">
<li>Create a router associated with the public “ext-net” network.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">external_router</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Neutron::Router</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">external_gateway_info</span><span class="p p-Indicator">:</span>
      <span class="l l-Scalar l-Scalar-Plain">network</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">ext-net</span>
</pre></div>
</div>
<ul class="simple">
<li>Attach a port from the network to the router.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">external_router_interface</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Neutron::RouterInterface</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">router</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">external_router</span> <span class="p p-Indicator">}</span>
    <span class="l l-Scalar l-Scalar-Plain">subnet</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">internal_subnet</span> <span class="p p-Indicator">}</span>
</pre></div>
</div>
<ul class="simple">
<li>Create a key pair called “HeatKeyPair.” Required property: name.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">ssh_keys</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Nova::KeyPair</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">name</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">HeatKeyPair</span>
    <span class="l l-Scalar l-Scalar-Plain">public_key</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">HeatKeyPair</span>
    <span class="l l-Scalar l-Scalar-Plain">save_private_key</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
</pre></div>
</div>
<ul class="simple">
<li>Create an instance using the “m1.small” flavor, “RHEL7” image, and
assign the subnet port created by “OS::Neutron::Port.”</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">instance_creation</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Nova::Server</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">flavor</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">m1.small</span>
    <span class="l l-Scalar l-Scalar-Plain">image</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">RHEL7</span>
    <span class="l l-Scalar l-Scalar-Plain">networks</span><span class="p p-Indicator">:</span>
      <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">port</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">subnet_port</span> <span class="p p-Indicator">}</span>
</pre></div>
</div>
<ul class="simple">
<li>Allocate an IP from the “ext-net” floating IP pool.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">floating_ip</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Neutron::FloatingIP</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">floating_network</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">ext-net</span>
</pre></div>
</div>
<ul class="simple">
<li>Allocate a a floating IP to the created instance from a
“instance_creation” function. Alternatively, a specific instance’s
ID can be defined here.</li>
</ul>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">floating_ip_association</span><span class="p p-Indicator">:</span>
  <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">OS::Nova::FloatingIPAssociation</span>
  <span class="l l-Scalar l-Scalar-Plain">properties</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">floating_ip</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">floating_ip</span> <span class="p p-Indicator">}</span>
    <span class="l l-Scalar l-Scalar-Plain">server_id</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">get_resource</span><span class="p p-Indicator">:</span> <span class="nv">instance_creation</span> <span class="p p-Indicator">}</span>
</pre></div>
</div>
</div>
<div class="section" id="parameters">
<h4><a class="toc-backref" href="#id99">Parameters</a><a class="headerlink" href="#parameters" title="Permalink to this headline">¶</a></h4>
<p>Parameters allow users to input custom variables for Heat templates.</p>
<p>Common options:</p>
<ul class="simple">
<li>type = The input type. This can be a string, number, JSON, a comma
separated list or a boolean.</li>
<li>label = String. The text presented to the end-user for the fillable
entry.</li>
<li>description = String. Detailed information about the parameter.</li>
<li>default = A default value for the parameter.</li>
<li>constraints = A parameter has to match a specified constraint. Any
number of constraints can be used from the available ones below.<ul>
<li>length = How long a string can be.</li>
<li>range = How big a number can be.</li>
<li>allowed_values = Allow only one of these specific values to be
used.</li>
<li>allowed_pattern = Allow only a value matching a regular
expression.</li>
<li>custom_constraint = A full list of custom service constraints can
be found at
<a class="reference external" href="#http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#custom-constraint">http://docs.openstack.org/developer/heat/template_guide/hot_spec.html#custom-constraint</a>.</li>
</ul>
</li>
<li>hidden = Boolean. Specify if the text entered should be hidden or
not. Default: false.</li>
<li>immutable = Boolean. Specify whether this variable can be changed.
Default: false.</li>
</ul>
<p>Syntax:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="l l-Scalar l-Scalar-Plain">parameters</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">&lt;CUSTOM_NAME&gt;</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">type</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">string</span>
        <span class="l l-Scalar l-Scalar-Plain">label</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;LABEL&gt;</span>
        <span class="l l-Scalar l-Scalar-Plain">description</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;DESCRIPTION&gt;</span>
        <span class="l l-Scalar l-Scalar-Plain">default</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;DEFAULT_VALUE&gt;</span>
        <span class="l l-Scalar l-Scalar-Plain">constraints</span><span class="p p-Indicator">:</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">length</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">min</span><span class="p p-Indicator">:</span> <span class="nv">&lt;MINIMUM_NUMBER&gt;</span><span class="p p-Indicator">,</span> <span class="nv">max</span><span class="p p-Indicator">:</span> <span class="nv">&lt;MAXIMUM_NUMBER&gt;</span> <span class="p p-Indicator">}</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">range</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">{</span> <span class="nv">min</span><span class="p p-Indicator">:</span> <span class="nv">&lt;MINIMUM_NUMBER&gt;</span><span class="p p-Indicator">,</span> <span class="nv">max</span><span class="p p-Indicator">:</span> <span class="nv">&lt;MAXIMUM_NUMBER&gt;</span> <span class="p p-Indicator">}</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">allowed_values</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span> <span class="nv">&lt;VALUE1&gt;</span><span class="p p-Indicator">,</span> <span class="nv">&lt;VALUE2&gt;</span><span class="p p-Indicator">,</span> <span class="nv">&lt;VALUE3&gt;</span> <span class="p p-Indicator">]</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">allowed_pattern</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;REGULAR_EXPRESSION&gt;</span>
            <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">custom_constrant</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;CONSTRAINT&gt;</span>
        <span class="l l-Scalar l-Scalar-Plain">hidden</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;BOOLEAN&gt;</span>
        <span class="l l-Scalar l-Scalar-Plain">immutable</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">&lt;BOOLEAN&gt;</span>
</pre></div>
</div>
<p>For referencing this parameter elsewhere in the Heat template, use this
syntax for the variable:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="p p-Indicator">{</span> <span class="nv">get_param</span><span class="p p-Indicator">:</span> <span class="nv">&lt;CUSTOM_NAME&gt;</span> <span class="p p-Indicator">}</span>
</pre></div>
</div>
<p>[66]</p>
</div>
</div>
<div class="section" id="vagrant">
<h3><a class="toc-backref" href="#id100">Vagrant</a><a class="headerlink" href="#vagrant" title="Permalink to this headline">¶</a></h3>
<p>Vagrant is a tool to automate the deployment of virtual machines. A
“Vagrantfile” file is used to initalize the instance. An example is
provided below.</p>
<div class="highlight-ruby notranslate"><div class="highlight"><pre><span></span><span class="nb">require</span> <span class="s1">&#39;vagrant-openstack-provider&#39;</span>

<span class="no">Vagrant</span><span class="o">.</span><span class="n">configure</span><span class="p">(</span><span class="s1">&#39;2&#39;</span><span class="p">)</span> <span class="k">do</span> <span class="o">|</span><span class="n">config</span><span class="o">|</span>

  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">box</span>       <span class="o">=</span> <span class="s1">&#39;vagrant-openstack&#39;</span>
  <span class="n">config</span><span class="o">.</span><span class="n">ssh</span><span class="o">.</span><span class="n">username</span> <span class="o">=</span> <span class="s1">&#39;cloud-user&#39;</span>

  <span class="n">config</span><span class="o">.</span><span class="n">vm</span><span class="o">.</span><span class="n">provider</span> <span class="ss">:openstack</span> <span class="k">do</span> <span class="o">|</span><span class="n">os</span><span class="o">|</span>
    <span class="n">identity_api_version</span>  <span class="o">=</span> <span class="s1">&#39;3&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">openstack_auth_url</span> <span class="o">=</span> <span class="s1">&#39;http://controller1/v3/auth/tokens&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">domain</span>             <span class="o">=</span> <span class="s1">&#39;default&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">username</span>           <span class="o">=</span> <span class="s1">&#39;openstackUser&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">password</span>           <span class="o">=</span> <span class="s1">&#39;openstackPassword&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">project_name</span>       <span class="o">=</span> <span class="s1">&#39;myProject&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">flavor</span>             <span class="o">=</span> <span class="s1">&#39;m1.small&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">image</span>              <span class="o">=</span> <span class="s1">&#39;centos&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">networks</span>           <span class="o">=</span> <span class="s2">&quot;vagrant-net&quot;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">floating_ip_pool</span>   <span class="o">=</span> <span class="s1">&#39;publicNetwork&#39;</span>
    <span class="n">os</span><span class="o">.</span><span class="n">keypair_name</span>       <span class="o">=</span> <span class="s2">&quot;private_key&quot;</span>
  <span class="k">end</span>
<span class="k">end</span>
</pre></div>
</div>
<p>Once those settings are configured for the end user’s cloud environment,
it can be created by running:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ vagrant up --provider<span class="o">=</span>openstack
</pre></div>
</div>
<p>[67]</p>
</div>
</div>
<div class="section" id="testing">
<h2><a class="toc-backref" href="#id101">Testing</a><a class="headerlink" href="#testing" title="Permalink to this headline">¶</a></h2>
<div class="section" id="tempest">
<h3><a class="toc-backref" href="#id102">Tempest</a><a class="headerlink" href="#tempest" title="Permalink to this headline">¶</a></h3>
<p>Tempest is used to query all of the different APIs in use. This helps to
validate the functionality of OpenStack. This software is a rolling
release aimed towards verifying the latest OpenStack release in
development but it should also work for older versions as well.</p>
<p>The sample configuration file “/etc/tempest/tempest.conf.sample” should be copied to “/etc/tempest/tempest.conf” and then modified. If it is not available then the latest configuration file can be downloaded from one of these sources: <a class="reference external" href="https://docs.openstack.org/tempest/latest/sampleconf.html">https://docs.openstack.org/tempest/latest/sampleconf.html</a> and <a class="reference external" href="https://docs.openstack.org/tempest/latest/_static/tempest.conf.sample">https://docs.openstack.org/tempest/latest/_static/tempest.conf.sample</a>.</p>
<ul>
<li><p class="first">Provide credentials to a user with the “admin” role.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[auth]</span>
<span class="na">admin_username</span>
<span class="na">admin_password</span>
<span class="na">admin_project_name</span>
<span class="na">admin_domain_name</span>
<span class="na">default_credentials_domain_name</span> <span class="o">=</span> <span class="s">Default</span>
</pre></div>
</div>
</li>
<li><p class="first">Specify the Keystone version to use. Valid options are “v2” and “v3.”</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[identity]</span>
<span class="na">auth_version</span>
</pre></div>
</div>
</li>
<li><p class="first">Provide the admin Keystone endpoint for v2 (uri) or v3 (uri_v3).</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[identity]</span>
<span class="na">uri</span>
<span class="na">uri_v3</span>
</pre></div>
</div>
</li>
<li><p class="first">Two different size flavor IDs should be given.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[compute]</span>
<span class="na">flavor_ref</span>
<span class="na">flavor_ref_alt</span>
</pre></div>
</div>
</li>
<li><p class="first">Two different image IDs should be given.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[compute]</span>
<span class="na">image_ref</span>
<span class="na">image_ref_alt</span>
</pre></div>
</div>
</li>
<li><p class="first">Define what services should be tested for the specific cloud.</p>
<div class="highlight-ini notranslate"><div class="highlight"><pre><span></span><span class="k">[service_available]</span>
<span class="na">cinder</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">neutron</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">glance</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">swift</span> <span class="o">=</span> <span class="s">false</span>
<span class="na">nova</span> <span class="o">=</span> <span class="s">true</span>
<span class="na">heat</span> <span class="o">=</span> <span class="s">false</span>
<span class="na">sahara</span> <span class="o">=</span> <span class="s">false</span>
<span class="na">ironic</span> <span class="o">=</span> <span class="s">false</span>
</pre></div>
</div>
</li>
</ul>
<p>[68]</p>
</div>
<div class="section" id="rally">
<h3><a class="toc-backref" href="#id103">Rally</a><a class="headerlink" href="#rally" title="Permalink to this headline">¶</a></h3>
<p>Rally is the benchmark-as-a-service (BaaS) that tests the OpenStack APIs for both functionality and for helping with performance tuning. This tool has support for using different verifier plugins. It is primarily built to be a wrapper for Tempest that is easier to configure and saves the results to a database so it can generate reports.</p>
<div class="section" id="installation">
<h4><a class="toc-backref" href="#id104">Installation</a><a class="headerlink" href="#installation" title="Permalink to this headline">¶</a></h4>
<p>Install Rally on RHEL. A specific GitHub branch or tag can be specified. Otherwise, the default “master” branch will be used. A target virtual environment directory can also be specified to isolate the installation.</p>
<p>RHEL:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ curl -L -o ~/install_rally.sh https://raw.githubusercontent.com/openstack/rally/0.10.1/install_rally.sh
$ sudo yum install gcc gmp-devel libffi-devel libxml2-devel libxslt-devel openssl-devel postgresql-devel python-devel python-pip redhat-lsb-core redhat-rpm-config wget
$ bash ~/install_rally.sh --branch <span class="m">0</span>.10.1 --target ~/rally-venv
</pre></div>
</div>
<p>Rally can now be used by activating the Python virtual environment.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span>$ . ~/rally-venv/bin/activate
</pre></div>
</div>
<p>Finish the installation by initializing a SQLite database for Rally. Alternatively, a MariaDB or PostgreSQL database connection can be configured in <code class="docutils literal notranslate"><span class="pre">~/rally-venv/etc/rally/rally.conf</span></code>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ rally db recreate
</pre></div>
</div>
<p>If Rally is ever upgraded to the latest version, the database schema also needs to be upgraded.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ rally db revision
<span class="o">(</span>rally-venv<span class="o">)</span>$ rally db upgrade
</pre></div>
</div>
<p>[70]</p>
</div>
<div class="section" id="registering">
<h4><a class="toc-backref" href="#id105">Registering</a><a class="headerlink" href="#registering" title="Permalink to this headline">¶</a></h4>
<p>Rally requires a configuration, that defines the OpenStack credentials to test with, is registered. It is recommended to use an account with the “admin” role so that all features of the cloud can be tested and benchmarked. The “admin” user is no longer required in Rally version &gt;= 0.10.0. [73]</p>
<p>View registered deployments:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ rally deployment list
<span class="o">(</span>rally-venv<span class="o">)</span>$ rally deployment show &lt;DEPLOYMENT_NAME&gt;
</pre></div>
</div>
<p><cite>1.</cite> Automatic</p>
<p>The fastest way to create this configuration is by referencing the OpenStack credential’s shell environment variables.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ . &lt;OPENSTACK_RC_FILE&gt;
<span class="o">(</span>rally-venv<span class="o">)</span>$ rally deployment create --fromenv --name<span class="o">=</span>existing
</pre></div>
</div>
<p><cite>2.</cite> Manual</p>
<p>A JSON file can be created to define the OpenStack credentials that Rally will be using. Example files can be found at <cite>~/rally-venv/samples/deployments/</cite>.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ cp ~/rally-venv/samples/deployments/existing.json ~/existing.json
</pre></div>
</div>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;devstack&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;auth_url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://&lt;KEYSTONE_ENDPOINT_HOST&gt;:5000/v3/&quot;</span><span class="p">,</span>
        <span class="nt">&quot;region_name&quot;</span><span class="p">:</span> <span class="s2">&quot;RegionOne&quot;</span><span class="p">,</span>
        <span class="nt">&quot;endpoint_type&quot;</span><span class="p">:</span> <span class="s2">&quot;public&quot;</span><span class="p">,</span>
        <span class="nt">&quot;admin&quot;</span><span class="p">:</span> <span class="p">{</span>
            <span class="nt">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;admin&quot;</span><span class="p">,</span>
            <span class="nt">&quot;password&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;PASSWORD&gt;&quot;</span><span class="p">,</span>
            <span class="nt">&quot;user_domain_name&quot;</span><span class="p">:</span> <span class="s2">&quot;admin&quot;</span><span class="p">,</span>
            <span class="nt">&quot;project_name&quot;</span><span class="p">:</span> <span class="s2">&quot;admin&quot;</span><span class="p">,</span>
            <span class="nt">&quot;project_domain_name&quot;</span><span class="p">:</span> <span class="s2">&quot;admin&quot;</span>
        <span class="p">},</span>
        <span class="nt">&quot;https_insecure&quot;</span><span class="p">:</span> <span class="kc">false</span><span class="p">,</span>
        <span class="nt">&quot;https_cacert&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;PATH_TO_CA_CERT&gt;&quot;</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For only using non-privileged OpenStack users, omit the “admin” dictionary. [72]</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="nt">&quot;openstack&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="nt">&quot;auth_url&quot;</span><span class="p">:</span> <span class="s2">&quot;https://&lt;KEYSTONE_ENDPOINT_HOST&gt;:5000/v3&quot;</span><span class="p">,</span>
        <span class="nt">&quot;region_name&quot;</span><span class="p">:</span> <span class="s2">&quot;RegionOne&quot;</span><span class="p">,</span>
        <span class="nt">&quot;endpoint_type&quot;</span><span class="p">:</span> <span class="s2">&quot;public&quot;</span><span class="p">,</span>
        <span class="nt">&quot;users&quot;</span><span class="p">:</span> <span class="p">[</span>
            <span class="p">{</span>
                <span class="nt">&quot;username&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;USER_NAME&gt;&quot;</span><span class="p">,</span>
                <span class="nt">&quot;password&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;PASSWORD&gt;&quot;</span><span class="p">,</span>
                <span class="nt">&quot;user_domain_name&quot;</span><span class="p">:</span> <span class="s2">&quot;Default&quot;</span>
                <span class="s2">&quot;project_name&quot;</span><span class="p">:</span> <span class="s2">&quot;&lt;PROJECT_NAME&gt;&quot;</span>
                <span class="s2">&quot;project_domain_name&quot;</span><span class="p">:</span> <span class="s2">&quot;Default&quot;</span>
            <span class="p">}</span>
        <span class="p">]</span>
    <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ rally deployment create --file<span class="o">=</span>~/existing.json --name<span class="o">=</span>&lt;DEPLOYMENT_NAME&gt;
</pre></div>
</div>
<p>[71]</p>
</div>
<div class="section" id="scenarios">
<h4><a class="toc-backref" href="#id106">Scenarios</a><a class="headerlink" href="#scenarios" title="Permalink to this headline">¶</a></h4>
<p>Scenarios define the tests that will be ran. Variables can be tweaked to customize them. All Rally scenario files are Jinja2 templates and can be in JSON or YAML format. Multiple scenarios can be setup in a single file for Rally to test them all.</p>
<p>Example scenarios:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ ls -1 ~/rally-venv/samples/tasks/scenarios/*
</pre></div>
</div>
<p>Each scenario can be configured using similar options.</p>
<ul>
<li><p class="first">args = Override default values for a task.</p>
</li>
<li><p class="first">context = Defines the resources that need to be created before a task runs.</p>
</li>
<li><p class="first">runner [74]</p>
<blockquote>
<div><ul>
<li><p class="first">concurrency (constant types) = The number of tasks to run at the same time (as different threads).</p>
</li>
<li><p class="first">duration (constant_for_duration type) = The number of seconds to run a scenario before finishing.</p>
</li>
<li><p class="first">max_concurrent (rps type) = The maximum number of threads that should spawn.</p>
</li>
<li><p class="first">rps (rps type) = The number of seconds to wait before starting a task in a new thread.</p>
</li>
<li><p class="first">times = The number of times the scenario should run.</p>
</li>
<li><p class="first">type</p>
<blockquote>
<div><ul class="simple">
<li>constant =  The number of <em>times</em> a scenario should run. Optionally run in parallel by setting the <em>concurrency</em>.</li>
<li>constant_for_duration = Run the scenario for a specified amount of time, in seconds, as defined by <em>duration</em>.</li>
<li>rps = Runs per second. Every <em>rps</em> amount of seconds a task is ran as a new thread.</li>
<li>serial = Specify the number of <em>times</em> to run a single task (without any concurrency support).</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">sla = “Service level agreement.” This defines when to mark a scenario as being failed.</p>
<blockquote>
<div><ul>
<li><p class="first">failure_rate</p>
<blockquote>
<div><ul class="simple">
<li>max = The number of times a task can fail before the scenario is marked as a failure.</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">max_seconds_per_iteration = The amount of seconds before a task is considered failed.</p>
</li>
</ul>
</div></blockquote>
</li>
</ul>
<p>After creating a scenario, it can be run from the CLI:</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ rally task start &lt;SCENARIO_FILE&gt;.&lt;JSON_OR_YAML&gt;
</pre></div>
</div>
<p>[71]</p>
</div>
<div class="section" id="reports">
<h4><a class="toc-backref" href="#id107">Reports</a><a class="headerlink" href="#reports" title="Permalink to this headline">¶</a></h4>
<p>All tasks that Rally runs are permanently stored in the database. The same detailed report that is sent to the standard output can also be viewed at any time after tasks are done running.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ rally task list
<span class="o">(</span>rally-venv<span class="o">)</span>$ rally task status &lt;TASK_UUID&gt;
<span class="o">(</span>rally-venv<span class="o">)</span>$ rally task detailed &lt;TASK_UUID&gt;
</pre></div>
</div>
<p>Reports can be generated in a “html” or “json” format. Multiple tasks can also be added to a single report.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ rally task report &lt;TASK_UUID_1&gt; &lt;TASK_UUID_2&gt; &lt;TASK_UUID_3&gt; --&lt;FORMAT&gt;
</pre></div>
</div>
<p>The JUnit XML (a Java unit test library) format can also be used. This library is not installed by default.</p>
<div class="highlight-sh notranslate"><div class="highlight"><pre><span></span><span class="o">(</span>rally-venv<span class="o">)</span>$ pip install junit-xml
<span class="o">(</span>rally-venv<span class="o">)</span>$ rally task <span class="nb">export</span> &lt;TASK_UUID&gt; --type junit
</pre></div>
</div>
<p>[78]</p>
</div>
</div>
</div>
<div class="section" id="performance">
<h2><a class="toc-backref" href="#id108">Performance</a><a class="headerlink" href="#performance" title="Permalink to this headline">¶</a></h2>
<p>OpenStack can be tuned to use less load and run faster.</p>
<ul>
<li><p class="first">KeyStone</p>
<blockquote>
<div><ul>
<li><p class="first">Switch to Fernet keys.</p>
<blockquote>
<div><ul class="simple">
<li>Creation of tokens is significantly faster because it does not rely on storing them in a database.</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Neutron</p>
<blockquote>
<div><ul>
<li><p class="first">Use distributed virtual routing (DVR).</p>
<blockquote>
<div><ul class="simple">
<li>This offloads a lot of networking resources onto the compute nodes.</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">General</p>
<blockquote>
<div><ul>
<li><p class="first">Utilize local DNS.</p>
<blockquote>
<div><ul class="simple">
<li>Ensure that all of the domain names in use are either available via a local recursive DNS server or on each server in the /etc/hosts file. This avoids a performance hit from external DNS lookups.</li>
</ul>
</div></blockquote>
</li>
<li><p class="first">Use memcache.</p>
<blockquote>
<div><ul class="simple">
<li>This is configured by an option called “memcache_servers” in the configuration files for most services. Consider using “CouchBase” for it’s ease of clustering and redundancy support.</li>
</ul>
</div></blockquote>
</li>
</ul>
</div></blockquote>
</li>
</ul>
</div>
<div class="section" id="errata">
<h2><a class="reference external" href="https://github.com/ekultails/rootpages/commits/master/src/openstack.rst">Errata</a><a class="headerlink" href="#errata" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="bibliography">
<h2><a class="toc-backref" href="#id110">Bibliography</a><a class="headerlink" href="#bibliography" title="Permalink to this headline">¶</a></h2>
<ol class="arabic simple">
<li>“OpenStack Releases.” OpenStack Releases. March 15, 2018. Accessed March 15, 2018. <a class="reference external" href="https://releases.openstack.org/">https://releases.openstack.org/</a></li>
<li>“New OpenStack Ocata stabilizes popular open-source cloud.” February 22, 2017. Accessed April 10, 2017. <a class="reference external" href="http://www.zdnet.com/article/new-openstack-ocata-stabilizes-popular-open-source-cloud/">http://www.zdnet.com/article/new-openstack-ocata-stabilizes-popular-open-source-cloud/</a></li>
<li>“Ocata [Goals].” OpenStack Documentation. April 10, 2017. Accessed April 10, 2017. <a class="reference external" href="https://governance.openstack.org/tc/goals/ocata/index.html">https://governance.openstack.org/tc/goals/ocata/index.html</a></li>
<li>“Pike [Goals].” OpenStack Documentation. April 10, 2017. Accessed April 10, 2017. <a class="reference external" href="https://governance.openstack.org/tc/goals/pike/index.html">https://governance.openstack.org/tc/goals/pike/index.html</a></li>
<li>“Queens [Goals].” OpenStack Documentation. September 26, 2017. Accessed October 4, 2017. <a class="reference external" href="https://governance.openstack.org/tc/goals/pike/index.html">https://governance.openstack.org/tc/goals/pike/index.html</a></li>
<li>“Red Hat OpenStack Platform Life Cycle.” Red Hat Support. January 24, 2018. <a class="reference external" href="https://access.redhat.com/support/policy/updates/openstack/platform">https://access.redhat.com/support/policy/updates/openstack/platform</a></li>
<li>“Frequently Asked Questions.” RDO Project. Accessed December 21, 2017. <a class="reference external" href="https://www.rdoproject.org/rdo/faq/">https://www.rdoproject.org/rdo/faq/</a></li>
<li>“How can I determine which version of Red Hat Enterprise Linux - Openstack Platform (RHEL-OSP) I am using?” Red Hat Articles. May 20, 2016. Accessed December 19, 2017. <a class="reference external" href="https://access.redhat.com/articles/1250803">https://access.redhat.com/articles/1250803</a></li>
<li>“Director Installation and Usage.” Red Hat OpenStack Platform 10 Documentation. November 23, 2017. Accessed December 22, 2017. <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/pdf/director_installation_and_usage/Red_Hat_OpenStack_Platform-10-Director_Installation_and_Usage-en-US.pdf">https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/pdf/director_installation_and_usage/Red_Hat_OpenStack_Platform-10-Director_Installation_and_Usage-en-US.pdf</a></li>
<li>“Project Navigator.” OpenStack. Accessed March 15, 2018. <a class="reference external" href="https://www.openstack.org/software/project-navigator/">https://www.openstack.org/software/project-navigator/</a></li>
<li>“Packstack: Create a proof of concept cloud.” RDO Project. Accessed March 19, 2018. <a class="reference external" href="https://www.rdoproject.org/install/packstack/">https://www.rdoproject.org/install/packstack/</a></li>
<li>“Neutron with existing external network. RDO Project. Accessed September 28, 2017. <a class="reference external" href="https://www.rdoproject.org/networking/neutron-with-existing-external-network/">https://www.rdoproject.org/networking/neutron-with-existing-external-network/</a></li>
<li>“Error while installing openstack ‘newton’ using rdo packstack.” Ask OpenStack. October 25, 2016. Accessed September 28, 2017. <a class="reference external" href="https://ask.openstack.org/en/question/97645/error-while-installing-openstack-newton-using-rdo-packstack/">https://ask.openstack.org/en/question/97645/error-while-installing-openstack-newton-using-rdo-packstack/</a></li>
<li>“OpenStack-Ansible.” GitHub. March 2, 2018. Accessed March 19, 2018. <a class="reference external" href="https://github.com/openstack/openstack-ansible">https://github.com/openstack/openstack-ansible</a></li>
<li>“[OpenStack-Ansible] Quickstart: AIO.” OpenStack  Documentation. March 26, 2018. Accessed March 26, 2018. <a class="reference external" href="https://docs.openstack.org/openstack-ansible/queens/user/aio/quickstart.html">https://docs.openstack.org/openstack-ansible/queens/user/aio/quickstart.html</a></li>
<li>“OpenStack-Ansible Deployment Guide.” OpenStack Documentation. March 19, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/">https://docs.openstack.org/project-deploy-guide/openstack-ansible/queens/</a></li>
<li>“Nova role for OpenStack-Ansible.” OpenStack Documentation. March 15, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/openstack-ansible-os_nova/queens/">https://docs.openstack.org/openstack-ansible-os_nova/queens/</a></li>
<li>“openstack ansible ceph.” OpenStack FAQ. April 9, 2017. Accessed April 9, 2017. <a class="reference external" href="https://www.openstackfaq.com/openstack-ansible-ceph/">https://www.openstackfaq.com/openstack-ansible-ceph/</a></li>
<li>“Configuring the Ceph client (optional).” OpenStack Documentation. April 5, 2017. Accessed April 9, 2017. <a class="reference external" href="https://docs.openstack.org/developer/openstack-ansible-ceph_client/configure-ceph.html">https://docs.openstack.org/developer/openstack-ansible-ceph_client/configure-ceph.html</a></li>
<li>“[OpenStack-Ansible] Operations Guide.” OpenStack Documentation. March 19, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/openstack-ansible/queens/admin/index.html">https://docs.openstack.org/openstack-ansible/queens/admin/index.html</a></li>
<li>“Developer Documentation.” OpenStack Documentation. March 19, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/openstack-ansible/latest/contributor/index.html">https://docs.openstack.org/openstack-ansible/latest/contributor/index.html</a></li>
<li>“[OpenStack-Ansible] Upgrade Guide.” OpenStack Documentation. March 19, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/openstack-ansible/queens/user/index.html">https://docs.openstack.org/openstack-ansible/queens/user/index.html</a></li>
<li>“TripleO quickstart.” RDO Project. Accessed March 26, 2018. <a class="reference external" href="https://www.rdoproject.org/tripleo/">https://www.rdoproject.org/tripleo/</a></li>
<li>“[TripleO] Minimum System Requirements.” TripleO Documentation. September 7, 2016. Accessed March 26, 2018. <a class="reference external" href="https://images.rdoproject.org/docs/baremetal/requirements.html">https://images.rdoproject.org/docs/baremetal/requirements.html</a></li>
<li>[RDO] Recommended hardware.” RDO Project. Accessed September 28, 2017. <a class="reference external" href="https://www.rdoproject.org/hardware/recommended/">https://www.rdoproject.org/hardware/recommended/</a></li>
<li>“[TripleO] Virtual Environment.” TripleO Documentation. Accessed September 28, 2017. <a class="reference external" href="http://tripleo-docs.readthedocs.io/en/latest/environments/virtual.html">http://tripleo-docs.readthedocs.io/en/latest/environments/virtual.html</a></li>
<li>“Getting started with TripleO-Quickstart.” OpenStack Documentation. Accessed December 20, 2017. <a class="reference external" href="https://docs.openstack.org/tripleo-quickstart/latest/getting-started.html">https://docs.openstack.org/tripleo-quickstart/latest/getting-started.html</a></li>
<li>“TripleO Documentation.” OpenStack Documentation. Accessed September 12, 2017. <a class="reference external" href="https://docs.openstack.org/tripleo-docs/latest/">https://docs.openstack.org/tripleo-docs/latest/</a></li>
<li>“Basic Deployment (CLI).” OpenStack Documentation. Accessed November 9, 2017. <a class="reference external" href="https://docs.openstack.org/tripleo-docs/latest/install/basic_deployment/basic_deployment_cli.html">https://docs.openstack.org/tripleo-docs/latest/install/basic_deployment/basic_deployment_cli.html</a></li>
<li>“Bug 1466744 - Include docker.yaml and docker-ha.yaml environment files by default.” Red Hat Bugzilla. December 13, 2017. Accessed January 12, 2018. <a class="reference external" href="https://bugzilla.redhat.com/show_bug.cgi?id=1466744">https://bugzilla.redhat.com/show_bug.cgi?id=1466744</a></li>
<li>“DevStack switching from MySQL-python to PyMySQL.” OpenStack nimeyo. June 9, 2015. Accessed October 15, 2016. <a class="reference external" href="https://openstack.nimeyo.com/48230/openstack-all-devstack-switching-from-mysql-python-pymysql">https://openstack.nimeyo.com/48230/openstack-all-devstack-switching-from-mysql-python-pymysql</a></li>
<li>“Using PostgreSQL with OpenStack.” FREE AND OPEN SOURCE SOFTWARE KNOWLEDGE BASE. June 06, 2014. Accessed October 15, 2016. <a class="reference external" href="https://fosskb.in/2014/06/06/using-postgresql-with-openstack/">https://fosskb.in/2014/06/06/using-postgresql-with-openstack/</a></li>
<li>“[Ceilometer] Installation Guide.” OpenStack Documentation. March 16, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/ceilometer/queens/install/">https://docs.openstack.org/ceilometer/queens/install/</a></li>
<li>“Liberty install guide RHEL, keystone DB population unsuccessful: Module pymysql not found.” OpenStack Manuals Bugs. March 24, 2017. Accessed April 3, 2017. <a class="reference external" href="https://bugs.launchpad.net/openstack-manuals/+bug/1501991">https://bugs.launchpad.net/openstack-manuals/+bug/1501991</a></li>
<li>“Message queue.” OpenStack Documentation. March 18, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/install-guide/environment-messaging.html">https://docs.openstack.org/install-guide/environment-messaging.html</a></li>
<li>“[oslo.messaging] Configurations.” OpenStack Documentation. March 19, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/oslo.messaging/queens/configuration/">https://docs.openstack.org/oslo.messaging/queens/configuration/</a></li>
<li>“ZeroMQ Driver Deployment Guide.” OpenStack Documentation. March 1, 2018. Accessed March 15, 2018. <a class="reference external" href="https://docs.openstack.org/oslo.messaging/latest/admin/zmq_driver.html">https://docs.openstack.org/oslo.messaging/latest/admin/zmq_driver.html</a></li>
<li>“[Keystone] Pike Series Release Notes.” OpenStack Documentation. Accessed March 15, 2018. <a class="reference external" href="https://docs.openstack.org/releasenotes/keystone/pike.html">https://docs.openstack.org/releasenotes/keystone/pike.html</a></li>
<li>“Setting up an RDO deployment to be Identity V3 Only.” Young Logic. May 8, 2015. Accessed October 16, 2016. <a class="reference external" href="https://adam.younglogic.com/2015/05/rdo-v3-only/">https://adam.younglogic.com/2015/05/rdo-v3-only/</a></li>
<li>“Install and configure [Keystone on RDO].” OpenStack Documentation. March 13, 2018. Accessed March 15, 2018. <a class="reference external" href="https://docs.openstack.org/keystone/queens/install/keystone-install-rdo.html">https://docs.openstack.org/keystone/queens/install/keystone-install-rdo.html</a></li>
<li>“OpenStack Keystone Fernet tokens.” Dolph Mathews. Accessed August 27th, 2016. <a class="reference external" href="http://dolphm.com/openstack-keystone-fernet-tokens/">http://dolphm.com/openstack-keystone-fernet-tokens/</a></li>
<li>“Ocata Series [Keystone] Release Notes.” OpenStack Documentation. Accessed April 3, 2017. <a class="reference external" href="https://docs.openstack.org/releasenotes/keystone/ocata.html">https://docs.openstack.org/releasenotes/keystone/ocata.html</a></li>
<li>“Hypervisors.” OpenStack Documentation. March 8, 2018. Accessed March 18, 2018. <a class="reference external" href="https://docs.openstack.org/nova/queens/admin/configuration/hypervisors.html">https://docs.openstack.org/nova/queens/admin/configuration/hypervisors.html</a></li>
<li>“Driving in the Fast Lane – CPU Pinning and NUMA Topology Awareness in OpenStack Compute.” Red Hat Stack. Mary 5, 2015. Accessed April 13, 2017. <a class="reference external" href="http://redhatstackblog.redhat.com/2015/05/05/cpu-pinning-and-numa-topology-awareness-in-openstack-compute/">http://redhatstackblog.redhat.com/2015/05/05/cpu-pinning-and-numa-topology-awareness-in-openstack-compute/</a></li>
<li>“CPU topologies.” OpenStack Documentation. March 8, 2018. Accessed March 18, 2018. <a class="reference external" href="https://docs.openstack.org/nova/queens/admin/cpu-topologies.html">https://docs.openstack.org/nova/queens/admin/cpu-topologies.html</a></li>
<li>“BLOCK DEVICES AND OPENSTACK.” Ceph Documentation. Accessed March 18, 2018. <a class="reference external" href="http://docs.ceph.com/docs/master/rbd/rbd-openstack">http://docs.ceph.com/docs/master/rbd/rbd-openstack</a></li>
<li>“Nested Virtualization in OpenStack, Part 2.” Stratoscale. June 28, 2016. Accessed November 9, 2017. <a class="reference external" href="https://www.stratoscale.com/blog/openstack/nested-virtualization-openstack-part-2/">https://www.stratoscale.com/blog/openstack/nested-virtualization-openstack-part-2/</a></li>
<li>“[Compute service] Overview.” OpenStack Documentation. March 8, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/nova/queens/install/overview.html">https://docs.openstack.org/nova/queens/install/overview.html</a></li>
<li>“Open vSwitch: Self-service networks.” OpenStack Documentation. March 16, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/deploy-ovs-selfservice.html">https://docs.openstack.org/neutron/queens/admin/deploy-ovs-selfservice.html</a></li>
<li>“Neutron Installation Guide.” OpenStack Documentation. March 16, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/neutron/queens/install/index.html">https://docs.openstack.org/neutron/queens/install/index.html</a></li>
<li>“DNS resolution for instances.” OpenStack Documentation. March 16, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/config-dns-res.html">https://docs.openstack.org/neutron/queens/admin/config-dns-res.html</a></li>
<li>“Introduction of Metadata Service in OpenStack.” VietStack. September 09, 2014. Accessed August 13th, 2016. <a class="reference external" href="https://vietstack.wordpress.com/2014/09/27/introduction-of-metadata-service-in-openstack/">https://vietstack.wordpress.com/2014/09/27/introduction-of-metadata-service-in-openstack/</a></li>
<li>“Load Balancer as a Service (LBaaS).” OpenStack Documentation. March 16, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/config-lbaas.html">https://docs.openstack.org/neutron/queens/admin/config-lbaas.html</a></li>
<li>“Quality of Service (QoS).” OpenStack Documentation. March 16, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/config-qos.html">https://docs.openstack.org/neutron/queens/admin/config-qos.html</a></li>
<li>“Neutron/DVR/HowTo” OpenStack Wiki. January 5, 2017. Accessed March 7, 2017. <a class="reference external" href="https://wiki.openstack.org/wiki/Neutron/DVR/HowTo">https://wiki.openstack.org/wiki/Neutron/DVR/HowTo</a></li>
<li>“Distributed Virtual Routing with VRRP.” OpenStack Documentation. March 16, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/neutron/queens/admin/config-dvr-ha-snat.html">https://docs.openstack.org/neutron/queens/admin/config-dvr-ha-snat.html</a></li>
<li>“BLOCK DEVICES AND OPENSTACK.” Ceph Documentation. Accessed March 26, 2018. <a class="reference external" href="http://docs.ceph.com/docs/master/rbd/rbd-openstack/">http://docs.ceph.com/docs/master/rbd/rbd-openstack/</a></li>
<li>“Volume encryption supported by the key manager.” Openstack Documentation. March 18, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/cinder/queens/configuration/block-storage/volume-encryption.html">https://docs.openstack.org/cinder/queens/configuration/block-storage/volume-encryption.html</a></li>
<li>“BLOCK DEVICES AND OPENSTACK.” Ceph Documentation. April 5, 2017. Accessed April 5, 2017. <a class="reference external" href="http://docs.ceph.com/docs/master/rbd/rbd-openstack/">http://docs.ceph.com/docs/master/rbd/rbd-openstack/</a></li>
<li>“Adding additional NAT rule on neutron-l3-agent.” Ask OpenStack. February 15, 2015. Accessed February 23, 2017. <a class="reference external" href="https://ask.openstack.org/en/question/60829/adding-additional-nat-rule-on-neutron-l3-agent/">https://ask.openstack.org/en/question/60829/adding-additional-nat-rule-on-neutron-l3-agent/</a></li>
<li>“Networking in too much detail.” RDO Project. January 9, 2017. Accessed February 23, 2017. <a class="reference external" href="https://www.rdoproject.org/networking/networking-in-too-much-detail/">https://www.rdoproject.org/networking/networking-in-too-much-detail/</a></li>
<li>“Upgrades.” OpenStack Documentation. January 15, 2017. Accessed January 15, 2017. <a class="reference external" href="http://docs.openstack.org/ops-guide/ops-upgrades.html">http://docs.openstack.org/ops-guide/ops-upgrades.html</a></li>
<li>“OpenStack Command Line.” OpenStack Documentation. Accessed October 16, 2016. <a class="reference external" href="http://docs.openstack.org/developer/python-openstackclient/man/openstack.html">http://docs.openstack.org/developer/python-openstackclient/man/openstack.html</a></li>
<li>“OpenStack Orchestration In Depth, Part I: Introduction to Heat.” Accessed September 24, 2016. November 7, 2014. <a class="reference external" href="https://developer.rackspace.com/blog/openstack-orchestration-in-depth-part-1-introduction-to-heat/">https://developer.rackspace.com/blog/openstack-orchestration-in-depth-part-1-introduction-to-heat/</a></li>
<li>“Heat Orchestration Template (HOT) specification.” OpenStack Documentation. November 17, 2017. Accessed November 17, 2017. <a class="reference external" href="https://docs.openstack.org/heat/latest/template_guide/hot_spec.html">https://docs.openstack.org/heat/latest/template_guide/hot_spec.html</a></li>
<li>“Heat Orchestration Template (HOT) specification.” OpenStack Developer Documentation. October 21, 2016. Accessed October 22, 2016. <a class="reference external" href="http://docs.openstack.org/developer/heat/template_guide/hot_spec.html">http://docs.openstack.org/developer/heat/template_guide/hot_spec.html</a></li>
<li>“Vagrant OpenStack Cloud Provider.” GitHub - ggiamarchi. January 30, 2017. Accessed April 3, 2017. <a class="reference external" href="https://github.com/ggiamarchi/vagrant-openstack-provider">https://github.com/ggiamarchi/vagrant-openstack-provider</a></li>
<li>“Tempest Configuration Guide.” Sep 14th, 2016. <a class="reference external" href="http://docs.openstack.org/developer/tempest/configuration.html">http://docs.openstack.org/developer/tempest/configuration.html</a></li>
<li>“Stable branches.” OpenStack Documentation. December 12, 2017. Accessed January 24, 2018. <a class="reference external" href="https://docs.openstack.org/project-team-guide/stable-branches.html">https://docs.openstack.org/project-team-guide/stable-branches.html</a></li>
<li>“[Rally] Installation and upgrades.” Rally Documentation. Accessed January 25, 2018. <a class="reference external" href="https://rally.readthedocs.io/en/latest/install_and_upgrade/index.html">https://rally.readthedocs.io/en/latest/install_and_upgrade/index.html</a></li>
<li>“[Rally] Quick start.” Rally Documentation. Accessed January 25, 2018. <a class="reference external" href="https://rally.readthedocs.io/en/latest/quick_start/index.html">https://rally.readthedocs.io/en/latest/quick_start/index.html</a></li>
<li>“Step 3. Benchmarking OpenStack with existing users.” OpenStack Documentation. July 3, 2017. Accessed January 25, 2018. <a class="reference external" href="https://docs.openstack.org/developer/rally/quick_start/tutorial/step_3_benchmarking_with_existing_users.html">https://docs.openstack.org/developer/rally/quick_start/tutorial/step_3_benchmarking_with_existing_users.html</a></li>
<li>“Allow deployment without admin creds.” OpenStack Gerrit Code Review. June 3, 2017. Accessed January 25, 2018. <a class="reference external" href="https://review.openstack.org/#/c/465495/">https://review.openstack.org/#/c/465495/</a></li>
<li>“Main concepts of Rally.” OpenStack Documentation. July 3, 2017. Accessed January 26, 2018. <a class="reference external" href="https://docs.openstack.org/developer/rally/miscellaneous/concepts.html">https://docs.openstack.org/developer/rally/miscellaneous/concepts.html</a></li>
<li>“[Ironic] Enabling drivers.” OpenStack Documentation. March 15, 2018. Accessed March 15, 2018. <a class="reference external" href="https://docs.openstack.org/ironic/queens/admin/drivers.html">https://docs.openstack.org/ironic/queens/admin/drivers.html</a></li>
<li>“VirtualBMC.” TripleO Documentation. Accessed January 29, 2018.</li>
<li>“CHAPTER 8. SCALING THE OVERCLOUD.” Red Hat Documentation. Accessed January 30, 2018. <a class="reference external" href="https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/html/director_installation_and_usage/sect-scaling_the_overcloud">https://access.redhat.com/documentation/en-us/red_hat_openstack_platform/10/html/director_installation_and_usage/sect-scaling_the_overcloud</a></li>
<li>“Verification reports.” Rally Documentation. Accessed March 13, 2018. <a class="reference external" href="http://docs.xrally.xyz/projects/openstack/en/latest/verification/reports.html">http://docs.xrally.xyz/projects/openstack/en/latest/verification/reports.html</a></li>
<li>“OpenStack Pike Repository.” CentOS Mirror. Accessed March 15, 2018. <a class="reference external" href="http://mirror.centos.org/centos-7/7/cloud/x86_64/openstack-pike/">http://mirror.centos.org/centos-7/7/cloud/x86_64/openstack-pike/</a></li>
<li>“External Ceph.” OpenStack Documentation. March 15, 2018. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/kolla-ansible/queens/reference/external-ceph-guide.html">https://docs.openstack.org/kolla-ansible/queens/reference/external-ceph-guide.html</a></li>
<li>“Containers based Undercloud Deployment.” OpenStack Documentation. Accessed March 19, 2018. <a class="reference external" href="https://docs.openstack.org/tripleo-docs/latest/install/containers_deployment/undercloud.html">https://docs.openstack.org/tripleo-docs/latest/install/containers_deployment/undercloud.html</a></li>
</ol>
</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="operating_systems.html" class="btn btn-neutral float-right" title="Operating Systems" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="networking.html" class="btn btn-neutral" title="Networking" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Luke Short. Documents licensed under GPLv3.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'2018.04.01',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>